Video Name,Transcript
Mod05_Sect01_ver2.mp4,"Hi, welcome back. This is Section 1 and we're going to introduce computer vision. Computer vision is an exciting space in machine learning. You can think of computer vision as the automated extraction of information from digital images. Using computer vision, machines can identify people, places, and things in images with an accuracy that's at or above human levels, and with greater speed and efficiency. Computer vision is often built with deep learning models. It automates the extraction, analysis, classification, and understanding of useful information from a single image or a sequence of images. The image data can take many forms, such as single images, video sequences, views from multiple cameras, or three-dimensional data. Computing power and algorithms have advanced over the last 10 years. This has led to an increase in capabilities and easier access to computer vision technologies. So how is computer vision being used? Here are some of the primary use cases for computer vision. You can use image and facial recognition to improve public safety and home security, or as a way to authenticate access to personal devices. You can also use it to automatically classify images for content management and analysis. Autonomous driving is partly enabled by computer vision technologies, and so are safety features of cars, such as lane detection or collision avoidance. Medical image analysis with computer vision can improve the accuracy and speed of a patient's medical diagnosis. This can result in better treatment outcomes and life expectancy for the patient. And finally, in manufacturing, well-trained computer vision is incorporated into robotics. This can improve quality assurance and operational efficiencies. These are just a few examples, and you can probably think of more. Computer vision problems can be broken down into a few areas. Content recognition is about identifying things in images. It's a classification problem, but it's a complex one with several layers. In the picture here, what's represented? Is it breakfast, lunch, or dinner? Would the classification only be food? The answer depends on what model you use to perform the classification. Models must be trained, and the training data provides the algorithm with data for it to learn from. Say that you have a model that was trained with pictures of different types of food. You might expect the image to output categories such as milk, peaches, mashed potato, chicken nuggets, and salad. If you trained the model with different images, it could classify objects as tray, cutlery, and napkin instead. When you work with images, you might want to know what kinds of objects are in the image and the location of those objects. Object detection provides the image categories, and where the objects are located in the image. There's a set of coordinates defining the location of a box surrounding the image, which is known as the bounding box. Bounding boxes for detection are typically top, left, width, and height coordinates surrounding the images. You can use these coordinates in your applications. When objects are detected in an image, there's a confidence number usually associated with that object. This percentage indicates the probability that the object belongs to a specific class. This confidence level is important when you want to determine an action that's based on object detection, especially in facial detection applications, or cases where the action has significance. Object segmentation is also known as semantic segmentation. It's like object detection, but you go into more detail to get fine boundaries for each detected object. Basically it's a fine-grained inference for predicting each pixel in the image. Some applications that require object segmentation include autonomous vehicles and advanced computer human interactions. Though object segmentation is a key problem in the field of computer vision, we won't be covering it in this course. Video adds another dimension to computer vision. In video, you get more data to work with, so you can capture the movement of people or objects which are referred to as instances. For example, you can detect people who enter and leave frames and also deal with moving cameras. Here's a use case for computer vision. Building on detection and tracking, you can analyze shopper behavior in your retail store by studying the path each person follows. If you use face analysis, you can understand other details about shoppers, such as average age ranges, gender distribution, and expressed emotions without identifying them. Here's another computer vision use case. You can also analyze images to identify actions using the motion in the video. For example, activities such as delivering a package or dancing. Looking at this image of a baseball player, some examples from the image could include capturing the batter's accuracy, the pitcher's pitching style, the type of pitch, slowball, slider, and others, the inning, and the batter's performance versus the specific picture. Managers could use all that data to coach players on how to improve their performance, and they do. Things can also use the data during the game to make game time decisions. Say you want to initiate various actions based on the speed of the baseball leaving the bat and its trajectory. A hit that's calculated by an ML model could lead to an audio or visual warning about a possible foul ball into the crowd. Or it could result in a preemptive alarm that a hit has a high probability of being a home run. This means that events following a home run could be both well timed and automated, such as playing music or setting off fireworks when the home run is hit by the home team. To wrap up this section, here are some key takeaways for this section. First, we covered how computer vision is the automated extraction of information from images. You can divide computer vision into two distinct areas, image analysis and video analysis. Image analysis includes object classification, detection, and segmentation. Video analysis includes instance tracking, action recognition, and motion estimation. Thanks for watching. We'll see you in the next video.  "
Mod02_Sect03.mp4,"Hi, and welcome back. This is Section 3, and we're going to give you a quick, high-level overview of machine learning terminology and a typical workflow. We will cover these topics in more detail later in this course, but for now, we'll focus on the larger picture. So to begin, you should always start with the business problem you or your team believe could benefit from machine learning. From there, you want to do some problem formulation. In this phase, one task is to articulate your business problem and convert it to an ML problem. After you've formulated the problem, you move on to the data preparation and preprocessing phase. You'll pull data from one or more data sources. These data sources might have differences in data or types that need to be reconciled so you can form a single cohesive view of your data. You'll need to visualize your data and use statistics to determine if the data is consistent and can be used for machine learning. We'll look at some of the data sources later in the course. This example data has four columns containing data from three different data sources. The sources had slightly different ways of representing data, and the results are shown in the table. In ML problems, columns represent features, and rows represent instances. There are some issues here with the data in some of the instances. In some cases, you'll need a subject matter expert or a functional expert to understand the authenticity of the data. For example, the date that's represented as 11 to 1969 could be November 2nd or February 11th in the year 1969. Someone who owns or manages the data pool would be able to clarify this ambiguity. Also, the word male can probably be attributed to an import issue where cells shifted position. But there could be an outside chance where it's the actual location, Male, a city that's the capital of the Republic of Maldives. At times, this error identification isn't as simple, and you'll need an SME to review the data. You'll learn about the role of experts later in this course. Remember that one of the largest impacts you can have on the success of a machine learning project is to have consistent and correct data. After your data is in good shape, it's time to train your model. This is where the process gets very iterative and fluid. You'll likely go through many multiple passes of feature engineering, training, evaluating and tuning before you find a model that meets your business goals. Feature engineering is the process of selecting or creating the features your model will be trained with. Features are the columns of data you have in your data set. The goal of the model is to correctly estimate the target value for the new data. The ML algorithm will use the features to predict the target. In this example, the target data is the average number of steps taken in a week. Selecting the correct features can involve adding, removing or calculating new features. You might want to make the data formats consistent. The consistent formats could be later used in the model, or you can make these changes for cosmetic reasons. Depending on the problem you want to solve at this data, you might not even need to include the name feature in the example data. What about the country feature? If this were a traditional database, you might want to move country to a lookup table, then reference it. Most ML algorithms want the data for an instance in a single row. ML algorithms need numerical data to process. You could consider turning the country text into the country's ISO code. However, the model might interpret the numerical value as having meaning. So the UK's ISO code value of 44 would be more significant than the ISO code value of the US, which is 0.1. In this case, splitting the data into multiple columns is fine. This is known as categorical encoding, and you'll learn about this later in the course. For other types of data, you could convert the text value into a numerical value. For example, you could use 0 or 1 to represent male or female. These numeric values can be used more easily by the model. What about the remaining features like age, birth month, which is shown as BM in the table, or day of week, which is shown as DOW? Extracting the age, birth month, and day of week might be appropriate depending on the problem you're trying to solve. Does age impact the target variable? What about which day of the week they were born on? Don't worry if this sounds complicated. You'll learn more about feature engineering later in this course. After your data is cleaned and you've identified the features you want to use, it's time to train a model. You won't use all the data to train your model. In fact, you need to hold some of the data so you have some data to test with. Typically, you'll use about 80% of the data to train with, and you'll save the rest of the data for testing. Next, you'll train a model with the training data. In the diagram, the model uses the XG boost algorithm. The model itself has some parameters you can set. These parameters will alter how the algorithm works and they're known as hyperparameters. The output of the training job will be a trained model. With the trained model, you can use some of the test data to see how well the model performs. You'll take an instance the model hasn't seen and use it to perform a prediction. Because you already know the target in your test data, you can compare the two values. From these comparisons, you can calculate metrics, which give you data on how well the model is performing. You'll then make changes to the model's data, features or hyperparameters until you find the model that yields the best results. When training your model, there's a real danger of overfitting or underfitting the model. Your model is overfitting your training data when you see the model performs well on the training data, but doesn't perform well on the evaluation data. This is because the model is memorizing the data it saw and can't generalize to unseen examples. Your model is underfitting the training data when the model performs poorly on the training data. This is because the model can't capture the relationship between the input examples, which are often called X, and the target values, which are often called Y. Understanding model fit is important for understanding the root cause of poor model accuracy. This understanding will guide you to take corrective steps. You can determine whether a predictive model is underfitting or overfitting the training data by looking at the prediction error on the training data and the evaluation data. We'll show you steps you can take to avoid this later in this course. After you've retrained the model and you're satisfied with the results, you deploy your model to deliver the best possible predictions. Later in this course, we'll walk you through these different phases and give you hands-on experience with each of them. But knowing the process is also useful when using the managed services we'll also explore later, where certain Amazon ML services will do the bulk of the work for you. Here are some of the key takeaways for this section. First, we looked at how the machine learning pipeline process can guide you through the process of training and evaluating a model. The iterative process can be broken into three broad steps, data processing, model training, and model evaluation. That's it for this video, we'll see you in the next one.  "
Mod05_Sect03_part3.mp4,"Hi, welcome back. We'll continue exploring video analysis by reviewing how to create the test data set. The final step before you train your model is to identify a test data set. You will use this test data set to validate and evaluate the model's performance. You'll do this by performing an inference on the images in the test data set. You'll then compare the results with the labeling information that's in the training data set. You can create your own test data set. Alternatively, you can use Amazon recognition custom labels to split your training data set into two data sets by using an 80-20 split. This split means that 80% of the data is used for training and 20% is used for testing. After you define the training and test data sets, Amazon recognition custom labels can automatically train the model for you. The service automatically loads and inspects the data, select the correct machine learning algorithms, trains a model, and provides model performance metrics. Your charge for the amount of time a model takes to train. A data set that contains more images and labels will take longer to train. When training is complete, you evaluate the performance of the model. During testing, Amazon recognition custom labels predicts if a test image contains a custom label. The confidence score is a value that quantifies the certainty of the model's prediction. Because this is a classification problem, the results can be mapped to a confusion matrix. With a true positive, the model correctly predicts the presence of the custom label in the test image. That is, the predicted label is also a ground truth label for that image. For example, Amazon recognition custom labels correctly returns a cat label when a cat is present in an image. For a false positive, the model incorrectly predicts the presence of a custom label in a test image. That is, the predicted label isn't a ground truth label for the image. For example, Amazon recognition custom labels returns a cat label, but there's no cat label in the ground truth for that image. For a false negative, the model doesn't predict that a custom label is present in the image, but the ground truth for that image includes this label. For example, Amazon recognition custom labels doesn't return a cat custom label for an image that contains a cat. With a true negative, the model correctly predicts that a custom label isn't present in the test image. For example, Amazon recognition custom labels doesn't return a cat label for an image that doesn't contain a cat. The console provides access to true positive, false positive, and false negative values for each image in your test data set. These prediction results are used to calculate the various metrics for each label and an aggregate of metrics for your entire test set. The same definitions apply to predictions that the model makes at the bounding box level. With bounding boxes, all metrics are calculated over each bounding box in each test image, regardless of whether the boxes are prediction or ground truth. To help you, Amazon recognition custom labels provides various metrics. For example, you can view summary metrics and evaluation metrics for each label. It also provides precision metrics for each label and an average precision metric for the entire test data set. Precision is the proportion of positive results that were correctly classified. Amazon recognition custom labels provides average recall metrics for each label and an average recall metric for the entire test data set. Recall is the fraction of your test set labels that were correctly classified. Using the previous example of cats, that would be how many cats were correctly classified. The service also provides an average model performance score for each label and an average model performance score for the entire test data set. The F1 score combines precision and recall together to give you just one number that quantifies the overall performance of a particular machine learning algorithm. You might use the F1 score when you have a class imbalance, but you also want to preserve the equality between precision and sensitivity. A higher value means better model performance for both recall and precision. If you're satisfied with the accuracy of your model, you can start using it. That's it for part three of this section. We'll see you again for part four, where we'll review how to evaluate and improve your model.  "
Mod04_Sect02_part1.mp4,"Hi, and welcome back. This is section 2, and we're going to focus on processing time series data, because it can be different from other types of data you've been using so far. Time series data is data that is captured in chronological sequence over a defined period of time. Introducing time into a machine learning model has a positive impact, because the model can derive meaning from changes in the data points over time. Time series data tends to be correlated. This means that there's a dependency between data points. This has mixed results for forecasting. This is because you're dealing with a regression problem, and regression assumes that data points are independent. You need to develop a method for dealing with data dependence so you can increase the validity of the predictions. In addition to the time series data, you can add related data to augmented forecasting model. For example, suppose you want to make a prediction about retail sales. You could include information about the product being sold, such as item identification, or sales price, along with the number of units sold per time period. The third type of data is metadata about the data set. For instance, say that you have a retail data set. You might want to include metadata, like a brand name, or a genre for music or videos, so you can group results. It's better to have more data. When you work with multiple data sources, you'll face the challenge of handling the time stamp of the data. You'll observe differences in the time stamp format and other challenges, such as incomplete data. However, you might be able to infer missing data in some cases. For example, say you have some data that contains both the month and the day, but no year. Observe whether the data seems to sequence through the month numbers in the database, repeating after 12. If it does so, you could add the year if you knew when the data started. You could then infer future years based on the order of the data. Much time stamp data is stored in UTC format, but not all data is. You should check if the time stamp is in local or universal time. Sometimes the time stamp doesn't represent the time you think it does. For example, suppose you have a database of cars that were serviced at a garage. Does the time stamp indicate the time the car arrived, was completed, or picked up? Or does it indicate when the final entry was entered into the system? Say you're trying to model the hourly, caloric intake of patients. However, you only have daily data. Then you'll need to adjust your target time scale. Also, your data might not have any time stamps. There could be other ways to extrapolate a time series depending on the data and domain. For example, you might have wavelength measurements or vectors within an image. As a final note, remember that daylight savings is different around the world. Also, because of daylight savings time might even occur twice a year in their time zones. A common occurrence in real world forecasting problems is missing values in the raw data. Missing values makes it harder for a model to generate a forecast. The primary example in retail is an out-of-stock situation in demand forecasting. If an item goes out of stock, the sales for the day will zero. If the forecast is generated based on those zero sales values, the forecast will be incorrect. There are many reasons why values can be marked as missing. Missing values can occur because of no transaction. They can also occur because of possible measurement errors. For example, a service that monitored certain data wasn't working correctly. Here's another example, the measurement couldn't happen correctly. In retail, the primary example for an inability to take correct measurements is an out-of-stock situation in demand forecasting. This means that demand doesn't equal sales on that day. There are several ways you can calculate the missing data. The first method is forward fill. This uses the last known value for the missing value. Moving on that idea, moving average uses the average of the last known values to calculate the missing value. Backward fill uses the next known value after the missing value. The danger here is that you're using the future to calculate the past, which is bad in forecasting. This method is also known as look ahead and should be avoided. Interpolation uses an equation to calculate the missing value. You can also use a zero fill. This is often used in retail because missing sales data shouldn't be calculated. The missing data represents that there were no orders on that day. It would be wise to investigate why this happened, but in this case, you don't want to fill in the missing value. You might get data at different frequencies. For example, you might have sales data that includes the exact timestamp the sale was recorded but have inventory data that only contains the year, month, and day of the inventory level. When you have data that's at a different frequency than other data sets or data that's not compatible with your question, you might need to down sample. Down sampling is moving from a more finely grained time to a less finely grained time. As the example shows, this could be converting an hourly data set to a daily data set. When down sampling, you need to decide how to combine the values. In the previous case of sales data, summing the quantity makes the most sense. If the data is temperature, you might want to find the average. Understanding your data helps you decide what's the best course of action. The opposite of down sampling is up sampling. When you move from a less finely grained time to a more finely grained time, the problem with up sampling is that it's extremely difficult to achieve in most cases. Suppose you want to up sample your sales data from daily sales to hourly sales. Unless you have some other data source to reference, you wouldn't be able to do this. There are cases when you need to do something, perhaps to match the frequency of another time series, or you might have an irregular time series or specific domain knowledge that would help. In those cases, you need to be careful how you make the conversion. For the retail example, the best you could do is create a single order for the day at a specified hour. For temperature, you could copy the daily temperature into each hourly slot or use a formula to calculate a curve. In data science, outliers have a mix of positive and negative attributes. The same is true of time series data. Suppose you were examining sales data, and you had an order that has an unusually high number of items. You might not want to include that in your forecast calculations, because the order size might never be repeated. Removing these outliers in anomalies is known as smoothing. Removing your data can help you deal with outliers and other anomalies. There are a few reasons why you might consider smoothing. First, during data preparation, you remove error values and could also remove outliers. You might also want to smooth your data to generate features. For visualization, you could smooth your data to reduce the noise and applaud. It's important to understand why you are smoothing the data and the impact it might have. The outcome might be to reduce noise and create a better model. But an equally important question is, could your smoothing compromise the model? Is the model expecting noisy data? Will you also be able to smooth the data in production? That's it for part one of this section. We'll see you again for part two where we'll review more time series specific challenges and the tools and algorithms that can help us wrangle your data.  "
Mod04_WrapUp.mp4,"Hi, welcome back. It's now time to review the module and wrap it up. In this module, you learned how to describe the business problem solved by Amazon Forecast, describe the challenges of working with time series data, list the steps required to create Forecast by using Amazon Forecast and use Amazon Forecast to make a prediction. Thanks for participating. See you in the next module.  "
Mod02_Sect01.mp4,"Hi and welcome to Section 1. In this section, we're going to talk about what machine learning is. This course is an introduction to machine learning, which is also known as ML. But first, we'll discuss where machine learning fits into the larger picture. Machine learning is a subset of artificial intelligence or AI. This is a broad branch of computer science that's focused on building machines that can do human tasks. Deep learning is a subdomain of machine learning. To understand where these all fit together, we'll discuss each one. As we just mentioned, machine learning is a subset of a broader computer science field known as artificial intelligence. AI focuses on building machines that can perform tasks a human would typically perform. In contemporary popular culture, you've probably seen AI's in movies, television, or works of fiction. For example, you might have seen AI's that control the world around them, or that start acting on their own initiative. These AI's started as computer agents that perceive their environments and take actions to achieve a specific goal. Though maybe not the outcome that created this originally wish-floor. Other fictional AI's interact extensively with humans as helpers or workers, and they generally do a better job working with humanity, but they're more general in purpose. These kinds of AI's are examples of artificial general intelligence or AGI. They have the capacity to learn or understand any task that a human can. AI problems typically span many fields of research, such as natural language processing, reasoning, knowledge representation, learning, perception, and physical environment interaction. AI isn't yet a reality for us unless we're all truly living in a simulation. But every year we move closer to it in each of those areas. You might have also read or seen commentary on the ethics of creating AI. Not all views are positive, perhaps partly in fear of the malicious fictional AI's that want to destroy humanity or use them as power sources. Or perhaps they're concerned about the risk of mass unemployment because an intelligent machine could work 24-7 and not need any breaks. Don't worry though, we're not going to build the next rogue AI in this course. Maybe in the next one. If you do a search, you'll probably find many definitions of machine learning. There isn't a universally agreed upon definition, so we'll start by looking at a couple of definitions. For example, we could say machine learning is the scientific study of algorithms and statistical models to perform a task by using inference instead of instructions. This isn't a bad starting point. The key point here is using algorithms and statistical models instead of instructions. To help you better understand this, we'll apply this idea to a concrete example. Suppose you need to write an application that determines if an email message is spam or not. Without machine learning, you'd need to write a complex series of decision statements using if and else statements. You'd also need to use words in the subject or body, the number of links, and the length of the message to determine if an email message is spam. It would be hard and labor intensive to build a large set of rules covering every possibility. With machine learning, however, you could use a list of email messages that were marked as spam or not spam and train a machine learning model. The model would then learn which patterns of words, length, and other attributes are good indicators of spam messages. If you presented the model with an email message it hadn't seen before, the model would perform a prediction to say whether the message was spam or not spam. Artificial learning represents a significant leap forward in the capabilities for artificial intelligence and machine learning. The theory behind deep learning was created from how the human brain works. An artificial neural network, or ANN, is inspired by the biological neurons found in the brain. Although the implementation has become very different. Artificial neurons have one or more inputs and a single output. These neurons fire or activate their outputs based on a transformation of the inputs. A neural network is composed of layers of these artificial neurons with connections between the layers. There are typically input, output, and hidden layers in the network. The output of a single neuron connects to the inputs of all the neurons in the next layer. The network is then asked to solve a problem. The input layer is populated from the training data, and the neurons activate throughout the layers until an answer is presented in the output layer. The accuracy of the output is then measured. If the output doesn't meet your threshold, the training is repeated, but with slight changes to the weights of the connections between neurons. The neural network will do this repeatedly. Each time it strengthens the connections that lead to success and diminishes the connections that lead to failure. As you'll see in this course, machine learning practitioners spend a lot of time optimizing the ML models, selecting the best data features to train with, and selecting the models with the best results. In contrast, deep learning practitioners spend almost no time on those tasks. Instead, they spend their time modeling data with different ANN architectures. Though the theory for deep learning goes back decades, the hardware needed to run deep learning problems wasn't generally accessible until recently. But now that it's available, you can use deep learning to address problems that are more complex than the problems you could have worked on before. Mainstream machine learning is a recent occurrence. Rapid advancements in machine and deep learning only started around the mid-2000s. This is partly because Moore's Law and the rise of cloud computing resulted in easier access to larger, faster, and cheaper compute and storage capabilities. You can now rent computing power for a few hours for pennies. Before this, you needed substantial investments to buy and operate large-scale compute clusters on your own. In 2012, neural networks started to be used in the ImageNet large-scale visual recognition challenge and machine learning competition for image recognition. The accuracy rate jumped up to about 82 percent and has been steadily climbing ever since. In fact, it exceeded human performance in 2015. Here are some of the key takeaways for this section. First, artificial intelligence is the broad field of building machines to perform human tasks. Also, machine learning is a subset of AI. It focuses on using data to train machine learning models so they can make predictions. Deep learning is a technique inspired from human biology. It uses layers of artificial neurons to build networks that solve problems. And last, advancements in technology, cloud computing, and algorithm development have led to a corresponding advance in machine learning capabilities and applications. That's it for this section. We'll see you in the next video.  "
Mod06_Sect01.mp4,"We'll get started by reviewing what natural language processing means. Natural language processing is also known as NLP. Before we explain what NLP is, we'll consider an example of NLP. Amazon Alexa. Alexa works by having a device, such as an Amazon Echo, record your words. The recording of your speech is sent to Amazon servers to be analyzed more efficiently. Amazon breaks down your phrase into individual sounds. Then, it connects to a database containing the pronunciation of various words to find which words most closely correspond to the combination of individual sounds. Amazon identifies important words to make sense of the tasks and carry out corresponding functions. For instance, if Alexa notices words like outside or temperature, it will open the weather Alexa skill. Amazon servers then send the information back to your device and Alexa speaks. NLP is a broad term for a general set of business or computational problems you can solve with machine learning or ML. However, NLP systems predate machine learning. For example, speech to text on older, pre-smart phone cell phones used NLP and so did screenreaders. Many NLP systems now use some form of machine learning. NLP considers the hierarchical structure of language. Words are at the lowest layer in a hierarchy. A group of words make a phrase. In the next level up, phrases make a sentence. And ultimately, sentences convey ideas. NLP systems face several significant challenges. We'll look at its challenges next. Language isn't precise. Words can have different meanings based on the other words that surround them. This is known as context. Often the same words or phrases can have multiple meanings. For example, consider the term weather. You could be under the weather, which has a colloquial meaning in English that you're sick. Or you could say there's wonderful weather outside, which means the weather conditions outside are good. The phrase, oh really, could convey surprise, disagreement, and many other things. It depends on the context and inflection. Here are some of the main challenges for NLP. One challenge is discovering the structure of the text. One of the first tasks of any NLP application is to break down the text into meaningful units, such as words, phrases, and sentences. Another challenge is labeling data. After the system converts the text to data, it must apply labels representing the various parts of speech. Every language will require a different labeling scheme to match the language's grammar. NLP also faces a challenge in representing context, because word meaning can depend heavily on context. Any NLP system needs a way to represent it. This is a large challenge because there are many contexts, and it's difficult to convert context into a form computers can understand. Finally, although grammar defines a structure for language, the application of grammar is indescribably large in scope. Handling the variation in how language is used by humans is a major challenge for NLP systems. That's where machine learning can have a large impact. You can apply NLP to a range of problems. Some of the more common applications include search applications, such as Google or Bing. And machine interactions like Alexa, sentiment analysis for marketing or political campaigns, social research based on media analysis, and chatbots to mimic human speech and applications. You can apply the machine learning development pipeline you've seen throughout this course when developing an NLP solution. The first task is to formulate a problem, then collect and label data. For NLP, collecting data consists of breaking down the text into meaningful subsets and labeling the sets. Feature engineering is a major part of NLP applications. This process gets complicated when you're dealing with highly irregular or unstructured text. For example, say you're building an application to classify documents. You'd need to be able to distinguish between the words with common terms, but different meanings. Labeling data in the NLP domain is sometimes also called tagging. In the labeling process, you assign individual text strings to different parts of speech. There are specialized tools you can use for NLP labeling. The first task for an NLP application is to convert the text to data so it can be analyzed. You convert text by removing words that aren't needed for analysis from the input text. In the example, the words this and is are removed to leave the phrase sample text. After removing stop words, you can normalize text by converting similar words into a common form. For example, the words run, runner, ran, and running are all different forms of the word run. You can normalize all instances of these words in a block of text using the stemming and limitization processes. Limitization groups different forms of a word into a single term. Limitization of the versions of the word run would group all instances of those forms into a single term, run. Removing on the other hand removes characters that the stemming algorithm considers unnecessary. Stemming might not work with the run example as the form ran might not be recognized as a form of the word run. After you've normalized the text, you can standardize it by removing words that aren't in the dictionary you're using for analysis. For example, you could remove acronyms, slang, and special characters. The natural language toolkit is also known as NLTK. Their Python library provides functions for removing stop words and normalizing text. Another first step in creating an NLP system is to convert the text into a data collection such as a data frame. All NLP libraries provide functions to assist with this process. The example shows using the word tokenize function from the NLTK library. After you've cleaned up your text and loaded it into a data frame, you can apply one of the NLP models to create features. Here are a couple of common models. The first model is known as bag of words. This is a simple model for capturing the frequency of words in a document. The model creates a key for each word. The value of the key is the number of times that word occurs in the document. The second model is term frequency and inverse document frequency, which is also known as TFIDF. Term frequency is a count of how many times a word appears in a document. Inverse document frequency is the number of times a word occurs in a group of documents. These two values are used together to calculate a weight for the words. Words that frequently appear in many documents have a lower weight. There are many established models in the NLP field. The example shows a bag of words model. Bag of words is a vector model. Vector models convert each sentence or phrase into a vector, which is a mathematical object that records both directionality and magnitude. In the example, a simple sentence is converted into a vector, where the frequency of each word is recorded. The word is has a value of two because it appears twice in the sentence. Bag of words is often used to classify documents into different categories. It's also used to derive attributes that feed into NLP applications, such as in sentiment analysis. There are three broad categories of text analysis. First, the classification of text is similar to other classification systems you've seen in this course. Text provides the input to a process that extracts features. Then you send the features through a machine learning algorithm that interacts with a classifier model and infers the classification. There are many applications for text matching. For example, autocorrect spelling and grammar checking are based on text matching. The algorithm for edit distance, also known as the Levinstein distance, is frequently used. You can derive relationships between different words or phrases in the text using a process called co-reference resolution. Several NLP systems provide Python libraries for deriving relationships. One of the biggest challenges for NLP is how to describe the context for the text. Consider this example where a user is searching for the term tablet. Because the word tablet has at least two distinct meanings, the search engine needs to know which meaning the user has in mind. Most search engines rely on the most commonly used context if the term isn't qualified further. For example, by adding another term like medicine or computing to the search, the process of extracting entities is known as named entity recognition or NER. And NER model has the following functions. First, it can identify noun phrases using dependency charts and part of speech tagging. It can classify phrases using a classification algorithm such as word to veck. Finally, it can disambiguate entities using a knowledge graph. Here's an example of using NER to extract the entities' Titanic and North Atlantic from the text. After the named entities are extracted, you can use a knowledge graph to extract meaning. A knowledge graph combines subject-matter expertise with machine learning to derive meaning. The Amazon Recommendations Engine is an example of a knowledge graph. Here are the main points to remember from this section. First, NLP predates machine learning. You can use the same NLP workflow that you've seen in other modules for NLP. Some of the main use cases for NLP are search query analysis, human machine interaction, and marketing and social research. NLP is complicated because human language lacks precision. Thanks for watching. We'll see you in the next video.  "
Mod01_Course Overview.mp4,"Hi, and welcome to Amazon Academy and Machine Learning Foundations. In this module, you'll learn about the course objectives, various job roles in the machine learning domain, and where you can go to learn more about machine learning. After completing this module, you should be able to identify course prerequisites and objectives, indicate the role of the data scientist in business, and identify resources for further learning. We're now going to look at the prerequisites for taking this course. Before you take this course, we recommend that you first complete AWS Academy Cloud Foundations. You should also have some general technical knowledge of IT, including foundational computer literacy skills like basic computer concepts, email, file management, and a good understanding of the internet. We also recommend that you have intermediate skills with Python programming and a general knowledge of applied statistics. Finally, general business knowledge is important for this course. This includes insight into how information technology is used in business. It's also important to have business-related skill sets such as communication skills, leadership skills, and an orientation towards customer service. In this course, you'll be introduced to the key concepts of machine learning, its tools and its uses. You'll also be introduced to and work with some of the AWS services for machine learning. You'll learn how to recognize how machine learning and deep learning are part of artificial intelligence. Describe artificial intelligence and machine learning terminology. Identify how machine learning can be used to solve a business problem. Describe the machine learning process. List the tools available to data scientists. And identify when to use machine learning instead of traditional software development methods. As part of this course, you'll also learn how to implement a machine learning pipeline. This includes how to formulate a problem from a business request, obtain and secure data for machine learning, build a Jupyter notebook by using Amazon SageMaker, outline the process for evaluating data, explain why data needs to be pre-processed, and use open source tools to examine and pre-process data. You will also use Amazon SageMaker to train and host a machine learning model. Use cross validation to test the performance of a machine learning model. Use a hosted model for inference. Create an Amazon SageMaker hyperparameter tuning job to optimize a model's effectiveness. And finally, how to use managed Amazon machine learning services to solve specific machine learning problems in forecasting, computer vision, and natural language processing. We'll now review the course outline. To achieve the course objectives, you'll complete the following modules. To start, in Module 2, you'll get an introduction to machine learning. In Module 3, you'll learn how to implement a machine learning pipeline with Amazon SageMaker. Modules 4, 5 and 6 describe how to apply managed Amazon machine learning services for problems in forecasting, computer vision, and natural language processing. Finally, Module 7 is a summary of the course. It also includes an overview of steps you can take to work towards the AWS certified machine learning specialty. The next five slides provide more detail about the subtopics covered in each module. The purpose of Module 2 is to introduce you to major concepts for understanding machine learning. Section 1 describes the overall field of machine learning and how machine learning relates to artificial intelligence and deep learning. In Section 2, you'll learn about some of the most common business problems you can solve with machine learning. Section 3 describes the general workflow for solving machine learning problems. You'll also learn some of the more common machine learning terms. In Section 4, you'll review some of the commonly used tools by machine learning professionals. And lastly, in Section 5, you'll get an overview of some of the common challenges you'll face when working with machine learning problems. In Module 3, you'll get an introduction to Amazon SageMaker and how you can use it to implement a machine learning pipeline. The module focuses on the application of machine learning to solve problems with several public domain data sets as examples of the machine learning pipeline. Section 1 introduces you to defining business problems and the data sets we'll use during this module. Section 2 through 8 describes the phases of the machine learning pipeline by using computer vision as an example application. In Section 2, you'll learn how to collect and secure data. Section 3 describes different techniques for evaluating data. In Section 4, you'll learn about the process of feature engineering. Section 5 describes the steps you'll take to train a model with SageMaker. In Section 6, you'll get an overview of the options in SageMaker for hosting and using a model. Finally, Section 7 and 8 cover how to evaluate and tune your model with SageMaker. In this module, you'll be introduced to using machine learning to create forecasts based on a time series data. In Section 1, you'll be introduced to forecasting in some of its common applications. Section 2 outlines some of the pitfalls of using time series data to make forecasts. Finally, in Section 3, you'll get an overview of how to use Amazon Forecast. In this module, you'll learn about using machine learning for computer vision. Section 1 describes the general problems you can solve with computer vision. In Section 2, you'll learn about the process for analyzing images and videos. And in Section 3, you'll learn the steps you'll need to take to prepare data sets for computer vision. In this module, you'll be introduced to natural language processing with machine learning. In Section 1, you'll learn about the general set of problems you can solve with natural language processing. Section 2 reviews some of the managed Amazon Machine Learning services you can use to address natural language processing problems. These services include Amazon Transcribe, Amazon Translate, Amazon Lex, Amazon Comprehend, Amazon Poly. Module 7 is the final module of the course. In this module, you'll review what you've learned throughout this course. You'll also be introduced to the next steps you should take if you want to achieve the AWS Certified Machine Learning specialty. Section 1 of this module summarizes the topics you've covered in this course. In Section 2, you'll learn more about the AWS documentation. You'll also review two common frameworks for applying AWS services. And finally, Section 3 describes the steps you should take if you want to continue working towards the AWS Certified Machine Learning specialty. In this section, you'll learn about some of the more common job roles for machine learning professionals. If you're interested in a data scientist role, focus on developing analytical, statistical and programming skills. As a data scientist, you'll use those skills to collect, analyze, and interpret large data sets. Some universities now offer degrees in data science, but data scientists often have degrees in related fields like statistics, math, computer science, or economics. As a data scientist, you'll need technical competencies in statistics, machine learning, programming languages, and data analytics. If you'd like to have a career as a machine learning engineer, the skills you'll need will be similar to a data scientist's skill set. Like data scientists, machine learning engineers also require technical competencies in statistics and machine learning. However, you'll focus more on programming skills and software architecture than analysis and interpretation. As a machine learning engineer, you'll apply those programming and architecture skills to design and develop machine learning systems. Machine learning engineers often have previous experience with software development, and they rely more heavily on programming and software engineering than other machine learning roles. You might also be interested in a career in science where you can apply machine learning technology to your field. Machine learning is having an impact in everything from astronomy to zoology, so there are many different paths open to you. As an applied science researcher, your primary focus will be on the type of science you're working on. You'll need some of the same skills as a data scientist, but you'll also need to know how to apply those skills to your chosen domain. Thus, applied science roles also require technical competencies in statistics and machine learning. Many software developers are now integrating machine learning into their applications. If you're interested in a career as a software developer, you should also include machine learning technology in your studies. As a machine learning developer, your primary focus will be software development skills. But you'll also need some of the same skills as a data scientist, so make sure you take coursework in statistics and applied mathematics. And here's a final note for this module. We recommend reviewing your student guide. In your student guide, you'll find links to documentation and other resources you'll use throughout the course. That's it for this introduction. Thanks for watching. We'll see you in the next video.  "
Mod05_WrapUp_ver2.mp4,"It's now time to summarize some of the main points in this module. In this module, you learned how to describe the use cases for computer vision. Describe the Amazon Managed Machine Learning Services available for image and video analysis. List the steps required to prepare a custom data set for object detection. Describe how Amazon SageMaker Ground Truth can be used to prepare a custom data set. And use Amazon Recognition to perform facial detection. That concludes this introduction to computer vision. Thanks for watching. We'll see you again in the next video.  "
Mod02_WrapUp.mp4,"It's now time to review the module. Here are the main takeaways for this module. First, we looked at defining machine learning and how it fits into the broader AI landscape. We also looked at the types of problem machine learning can help us solve and how machine learning applies learning algorithms to develop models from large data sets. We then looked at the machine learning pipeline and the different stages for developing a machine learning application. Finally, we introduced some of the tools and services you can use before discussing some of the challenges with machine learning. In summary, in this module, you learned how to recognize how machine learning and deep learning are part of artificial intelligence. Describe artificial intelligence and machine learning terminology. Denify how machine learning can be used to solve a business problem. Describe the machine learning process. List the tools available to data scientists. Identify when to use machine learning instead of traditional software development methods. Thanks for watching. We'll see you in the next video.  "
Mod05_Sect02_part1_ver2.mp4,"Welcome back. In this section, we'll explore image analysis in more detail, and in part two, we'll take a closer look into video analysis. To start, we'll introduce the main Amazon service we'll be using, Amazon Recognition. Amazon Recognition is a computer vision service that's based on deep learning. You can use it to add image and video analysis to your applications. There are many uses for Amazon Recognition, including creating searchable image and video libraries. Amazon Recognition makes both images and stored videos searchable, so you can discover the objects and scenes that appear in them. You can use Amazon Recognition to build a face-based user verification system, so your applications can confirm user identities by comparing their live image with a reference image. Amazon Recognition interprets emotional expressions such as happy, sad, or surprise. It can also interpret demographic information from facial images, such as gender. Amazon Recognition can also detect inappropriate content in both images and stored videos. And finally, Amazon Recognition can recognize and extract text content from images. Before we go further, here's a quick note on security. You need to check if the applications you build using Amazon Recognition fall under any regulatory restrictions as defined in your field or country. Security and compliance for Amazon Recognition is a shared responsibility between AWS and the customer. For more information about this topic, see the AWS Compliance page. Amazon Recognition is an AWS managed service. With a managed service, Amazon hosts the machine learning models, maintains an API, and scales out to meet demand for you. You can benefit from a set of models that constantly learn and improve. Also, you can focus on building applications that use the API and optionally training the service to understand your unique business needs. There are various resources you can use to access and interact with Amazon Recognition, such as APIs, SDKs, and commands for the AWS Command Line Interface, which is also known as the AWS CLI. The languages supported by the SDKs include JavaScript, Python, PHP, .NET, Ruby, Java, Go, Node.js, and C++. Finally, Amazon Recognition integrates with other AWS services. For example, if you need storage, you can use Amazon Simple Storage Service or S3. For authentication and authorization, you can use AWS Identity and Access Management, which is also known as IAM. This diagram illustrates an image search feature where users can take pictures and get information about the real estate properties they're viewing. First, the user takes a picture with their mobile device. The user then initiates a search, which causes the application to upload to Amazon S3. S3 is configured to call other services when a write event occurs. In this case, the bucket passes the S3 path of the new object to AWS Lambda. When the Lambda function is called, it uses the Amazon Recognition SDK to call the service. Amazon Recognition analyzes the image, detects aspects of the property, creates labels, and passes the information back to Lambda as an object formatted in JavaScript Object notation or JSON. Lambda then stores the labels and confidence score in Amazon Elastic Search Service, which is also known as Amazon ES. Application users can now identify aspects of a property using the objects that were detected in the image. In this example architecture, the system checks uploaded images for inappropriate content. Like the previous example, processing begins when the user uploads content. First, the user uploads an image to Amazon S3. Second, the S3 bucket is configured to call a Lambda function when an object is written to the bucket. Third, Lambda calls Amazon Recognition via the SDK. Amazon Recognition then analyzes the images for inappropriate content and sends the response back to Lambda. Fourth, if the content is appropriate, the content is approved. Fifth, if the content isn't appropriate, the content can be sent for manual inspection. And finally, if the content isn't approved, a notification is sent to the user. In this final use case, the system analyzes a video feed for sentiment analysis. First, an in-store camera captures video that's then sent to a back office or a cloud-based application. Typically, an application like this uses Amazon Kinesis to stream the video. Second, the application uses the SDK to send the video to Amazon Recognition for further analysis. Visual sentiment is extracted along with other attributes such as age. Third, the discovered attributes are sent to Amazon Kinesis. Fourth, a Lambda function extracts the data from the stream. Fifth, the data is then written to S3. Next, the data is loaded into Amazon Redshift on a regular basis. And finally, tools like Amazon QuickSight can be used to generate reports from the data. Amazon Recognition is designed to integrate into your applications through the API and SDKs. API operations are provided for detecting labels, faces, recognizing celebrities, and detecting unsafe images. To perform a prediction, provide the service with an image object in Amazon S3 or upload a byte stream of an image. Images can be in JPEG or PNG formats. Amazon Recognition processes the image, performs the prediction, and returns a JSON object with the results. When Amazon Recognition performs predictions, it often returns multiple labels. Each label has a confidence level. This confidence level indicates how likely the label was found in the image. Like this example shows, labels can also have hierarchies. When you find instances of objects, you need to understand where the detected object is in the image. For each instance, the results from Amazon Recognition include a bounding box that contains the starting coordinate of top, left, and box dimensions of width, height. Like the example, you can use this information to determine the location of the detected object in the image. It's important to note that all findings contain a confidence score. You can use the confidence score in your applications to tune your response to predictions. With a higher score, it's more likely that the object was correctly labeled. That's it for part one of this section. We'll see you again for part two where we'll explore facial detection.  "
Mod05_Sect03_part4_ver2.mp4,"Hi, welcome back. We'll continue exploring video analysis by reviewing how to evaluate and improve your model. In general, you can improve the quality of your model with larger quantities of better quality data. Use training images that clearly show the object or scene and don't include many things that you're not interested in. For bounding boxes around objects, use training images that show the object as fully visible and not hidden by other objects. Make sure that your training and test data sets match the type of images that you'll eventually run inference on. For objects where you have just a few training examples, like logos, you should provide bounding boxes around the logo in your test images. These images represent the scenarios you want to localize the object in. Reducing false positives often results in better precision. To reduce false positives, first, check if increasing the confidence threshold enables you to keep the correct predictions while eliminating false positives. Increasing the confidence threshold eventually results in diminishing gains because of the trade-off between precision and recall for a given model. Next, check to see if you need to add additional classes for training. For example, if you are detecting cats but often dogs are being flagged as cats, add dog as a label to your training data set, along with the images of dogs that you got the false positive on. Effectively, you're helping the model learn to predict dog and not cat through the new training images. You might find that the model is confused between two of your custom labels, cat and dog. The test image with label cat is predicted as having labeled dog in vice versa. In this case, first, check for mislabeled images in your training and test sets. Also adding more training images that reflect this confusion will help a retrain model learn to better discriminate between cat and dog. Using false negatives often results in better recall. To reduce false negatives, first, lower the confidence threshold. This should improve recall. Also use better examples to model the variety of both the object and the images they appear in. Finally, split your label into two classes that are easier to learn. For example, instead of good cookies and bad cookies, you might want good cookies, burnt cookies, and broken cookies to help the model learn each unique concept better. If you're satisfied with the performance of your model, you can make it available for use by starting it from the console or by using code. After the model is running, you can perform an inference with the AWS CLI or the SDK. When you call the API, you specify the Amazon resource name of the Amazon recognition custom labels model that you want to use. The Amazon resource name is also known as an ARN. You'll also specify the image you want the model to make a prediction with. You can provide an input image as an image byte array of base 64 encoded image bytes, or as an S3 object. Custom labels are returned in an array of custom label objects. Each custom label represents a single object, scene or concept that's found in the image. A custom label includes a label for the object, scene or concept that was found in the image. It also includes a bounding box for objects that were found in the image. The bounding box coordinates show where the object is located on the source image. The coordinate values are a ratio of the overall image size. Finally, the custom label includes the confidence score. This represents how confident Amazon recognition custom labels is in the accuracy of the label and bounding box. During training, a model calculates a threshold value that determines if a prediction for a label is true. By default, the Detect Custom Labels operation doesn't return labels with a confidence value that's less than the model's calculated threshold value. To filter the returned labels, specify a value for min confidence that's greater than the model's calculated threshold. You can get the model's calculated threshold from the model's training results in the Amazon recognition custom labels console. To get all the labels regardless of confidence, specify a min confidence value of zero. If you find that the confidence values returned by the Detect Custom Labels operation are too low, consider retraining the model. You can restrict the number of custom labels that are returned from the Detect Custom Labels operation by specifying the max results input parameter. The returned results are sorted from the highest confidence to the lowest confidence. Here are some key takeaways from this section of the module. Models must be trained for the specific domain that you want to analyze. If you're looking for turbochargers, you'll need many pictures of turbochargers to train your model. You can set custom labeling for the specific business case. We looked at the custom labeling process and some of the tools you can use. If you want objects to be detected, you need to label images and create bounding boxes for these objects. You can use Amazon SageMaker GroundTruth to build training data sets for your models, which can also use machine learning to label your images. Thanks for watching and we'll see you in the next video.  "
Mod06_WrapUp.mp4,"Welcome back. It's now time to review the module and wrap it up. In summary, in this module you learn how to describe the NLP use cases that are solved by using Managed Amazon ML Services and describe the Managed ML Services available for NLP. Good job! Thanks for watching, we'll see you in the next module.  "
Mod03_Sect01.mp4,"Hi, and welcome back to Module 3. This is Section 1, and we're going to take a look at some of the data sets we'll use in this module. We'll also look at guidance for how to formulate a business problem. Before we get started, here's a reminder of the machine learning pipeline we looked at in the previous module, and how that maps to the sections in this module. This section, Section 1, will cover how to formulate a problem. It will also cover the data sets we'll use throughout this module. Section 2 will discuss how to obtain and secure data for your machine learning activities. In Section 3, we'll show you tools and techniques for gaining and understanding of your data. Then, in Section 4, we'll look at pre-processing your data, so it's ready to train a model. Section 5 will cover selecting and training an appropriate machine learning model. Section 6 will show you how to deploy a model so you can make a prediction. Section 7 will examine the process of evaluating the performance of a machine learning model. And finally, in Section 8, we'll look at tuning the model. The machine learning pipeline is an iterative process. When you work on a real-world problem, you might find yourself iterating many times before you arrive at a solution that meets your business's needs. In this first section, we'll examine how to think about turning a business requirement into a machine learning problem. The first step in this phase is to simply define the problem you want to solve and the goal you want to reach. Understanding the business goal is key because you'll use it to measure the performance of your solution. It's not unusual to solidify the business problem before you can begin targeting a solution. There are a lot of other questions you can ask to develop a good understanding of the problem. With more information about the problem, you can begin framing an approach. First, can the problem even be solved by machine learning? Or would a traditional approach make more sense? Is this a supervised or unsupervised machine learning problem? Do you have labeled data to train a supervised model? There are many questions you could ask yourself and the business. Ultimately, you should try to validate the use of machine learning and you should make sure you have access to the right people and data. You should also try to come up with the simplest solution to the problem. Here's an example. You want to identify fraudulent credit card transactions so you can stop the transaction before it processes. That's your problem. Now, what's the business goal or outcome driving this problem statement? In this case, say that the intended outcome is a reduction in the number of customers who end their membership to the credit card as a result of a fraudulent transaction. From a business perspective, how do you define success given this problem in the desired outcome? This is the stage where you need to move from qualitative statements to quantitative statements that can be easily measured. Depending on the example, a metric you could use to define success with this problem might be a 10% reduction in the number of customers who file claims for fraudulent transactions within a six month period. So now you've defined the business side of your problem. It's time to start thinking about this in terms of your machine learning model itself. What's the actual output you want to see from your model? You want to be specific here. It should be a statement that reflects what an ML model could actually output. An example might be the model will output whether or not a credit card transaction is fraudulent or not fraudulent. Now that you know what you want your ML model to actually achieve, you can use this information to determine the type of ML you're working with. If you have historical data where customers file reports for fraud transactions, you can use this data for your machine learning purpose. This historical data falls under the supervised learning approach where the labels are already defined. Recall from earlier in this course that supervised ML types are categorized into two groups, classification and regression. In the credit card example, the desired output of categorizing a transaction is fraud or not fraud. So you can see that you're dealing with a binary classification problem. Throughout this module, you'll see several data sets being used. You can access these data sets and many more from the UC Irvine machine learning repository. The first data set contains numerical information about the composition of wine, along with the quality of the wine. The question you might want to ask on this data set is, based on the composition of the wine, could we predict the quality and therefore the price? In addition to that question, we'll also use this data set to view statistics, deal with outliers and scale numerical data. The second data set is a car evaluation database. This data set is heavily text based. This enables you to explore the encoding categorical data, which converts the text values into numbers that can be processed by machine learning. The third data set is a biomedical data set, which you'll also use in the labs. The question to answer for this data set is, based on the biomechanical features, can you predict if a patient has an abnormality? This data set will take you through the entire end-to-end process. You'll end with a trained model that's been tuned and that you can use to make a prediction. In this section, we looked at how business problems need to be converted into an ML problem. We also looked at some of the key questions to ask, which are, what is defining success? Can you measure the outcome or impact if your solution is implemented? Most business problems fall into one of two categories. The first category is classification, which can be binary or multi-class. Ask yourself, does the target belong to a class? And the second category is regression. For this, ask yourself, can you predict a numerical value? That's it for section one. We'll see you in the next video.  "
Mod05_Sect02_part2.mp4,"Hi, welcome back. We'll continue exploring image analysis with a closer look at facial detection. Facial detection uses a model that was tuned to perform predictions specifically for detecting faces and facial features. Facial detection has many of the same features, a standard object detection, such as a bounding box or the coordinates of the box surrounding the face that was detected. This will include a value representing the confidence that the bounding box contains a face. There will be a list of attributes, if found, such as if the face has a beard or if it appears to be male or female. There will also be a confidence score for these attributes. It can also detect physical emotions like whether the person is smiling or frowning. It's important to understand this classification is based only on visual clues, and so it might not represent the actual emotion of the person. Facial landmarks are components of the face, such as eyes and mouth. Typical landmarks also include x and y coordinates. Quality describes the brightness and the sharpness of the face, and pose describes the rotation of the face inside the image. And confidence is a feature here, and it's provided for each detected feature. And remember, the feature prediction is based only on visual observations. With Amazon recognition, you can compare two images to determine if they contain the same person. Comparisons require both a source and a target image. The results will include all the faces that were found, and they include information about matching and non-matching faces. Again, confidence scores indicate how likely each prediction is. Amazon recognition can also search for known faces. To use this feature, you need to train the model by providing a collection of images to use. After you train the model, you can then detect those people and images you provide. To find known faces, first, create a collection, and add faces to the collection. Amazon recognition will perform facial recognition on the images you provide. It will then return typical information, like the bounding box coordinates or the confidence score. To associate faces with an image, specify an image ID in the external image ID request parameter. This could be the file name of the image or another ID that you create. After you create your collection, you can then use the search faces by image operation to search for faces from the collection. The return data contains an array of all faces that matched. The information includes bounding boxes, confidence scores, and the external image ID value. You can then use this ID value to link back to the source image. Now that you've learned about the facial detection features of Amazon recognition, here's a summary of the guidelines we've discussed so far. When Amazon recognition detects a human face, it captures a bounding box that shows where the face was found in the video. It can also detect attributes such as the position of the eyes, nose, and mouth. It can detect emotion, the quality of the detection, and any landmarks that might appear. All these items will have an associated confidence score. A higher score means that the model has greater confidence about the detection. Gender is inferred from the image, not inferred from identity. Similarly, emotion is also determined from the image, and it might not reflect the subject's actual emotional state. How should I apply facial recognition responsibly? Facial recognition should never be used in a way that violates an individual's rights, including the right to privacy. It should also never be used to make autonomous decisions for scenarios that require a human being to analyze them. For example, suppose that a bank uses tools like Amazon recognition in a financial application to verify their customer's identities. The bank should always clearly disclose the use of the technology and ask the customer to approve their terms and conditions. For more information about this topic, see the AWS webpage about the facts on facial recognition with artificial intelligence. We'll now explain how you can use Amazon recognition to process videos. You can perform video processing on both stored videos and video streams. Stored videos should be uploaded and stored in an S3 bucket. Each type of detection has its own start operation. You can search for people, faces, labels, celebrities, text, and inappropriate content. Amazon recognition publishes a completion status to a topic in Amazon Simple Notification Service, which is also known as Amazon SNS. Then, SNS can route these messages to subscribers. For durability, it's a best practice to route messages to a message queue in Amazon Simple Queue Service or Amazon SQS. Your application should monitor the SQS queue for completion. Each start operation has a corresponding GET operation for retrieving the results. If you call GET Detection Results, it returns an array of labels that contain information about any labels found in the video. The label information includes the same labels as image detection, but it also includes a timestamp of where the label was detected in milliseconds from the start of the video. In addition to stored videos, you can also use Amazon recognition video to detect and recognize faces in streaming video. A typical use case for this is detecting a known face in a video stream. Amazon recognition video uses Amazon Kinesis video streams to receive and process a video stream. The analysis results are output from Amazon recognition video to a Kinesis data stream. They are then read by your client application. Amazon recognition video provides a stream processor that's called create stream processor, and you can use it to start and manage the analysis of the streaming video. To use Amazon recognition video with your streaming video, your application must implement these resources. First, you need a Kinesis video stream to send streaming video to Amazon recognition video. Next, you need an Amazon recognition video stream processor to manage the streaming video analysis. And finally, you need a Kinesis data stream consumer to read the analysis results that Amazon recognition video sends to the data stream. If you want to find a face in a video, you need to create a collection. This process is the same as creating a collection for still images. Amazon recognition video places a JSON frame record for each analyzed frame into the Kinesis output stream. Amazon recognition video doesn't analyze every frame that's passed to it through the Kinesis video stream. A frame record that sent to a Kinesis data stream contains information about which video stream fragment the frame is in, where the frame is in the fragment, and faces that are recognized in the frame. It also includes status information for the stream processor. Before we wrap up, here's a quick summary. Amazon recognition is a computer vision service that's based on deep learning. You can easily add image and video analysis to your applications. Amazon recognition can detect faces, sentiment, text, unsafe content, and library search in both images and video. Amazon recognition is integrated with other AWS services. Thanks for watching. We'll see you in the next video.  "
Mod03_WrapUp.mp4,"It's now time to review the module and wrap up with a knowledge check. In this module, you learned how to formulate a problem from a business request. Obtain and secure data for machine learning. Build a Jupyter Notebook by using Amazon SageMaker. Outline the process for evaluating data. Using why data needs to be pre-processed. Use open source tools to examine and pre-process data. Use Amazon SageMaker to train and host a machine learning model. Use cross validation to test the performance of an ML model. Use a hosted model for inference. Create an Amazon SageMaker hyper parameter tuning job to optimize a model's effectiveness. That concludes this module. Thanks for watching. We'll see you again in the next video.  "
Mod03_Intro.mp4,"Welcome back to AWS Academy Machine Learning. This is Module 3, and we're going to work through the entire machine learning pipeline by using Amazon SageMaker. This module will discuss a typical process for handling a machine learning problem. The machine learning pipeline can be applied to many machine learning problems. The focus is on supervised learning, but the process you learn in this module can be adapted to other types of machine learning as well. This is a large module, and we'll be covering a lot of material. At the end of this module, you'll be able to formulate a problem from a business request, obtain and secure data for machine learning. Until they jupiter notebook by using Amazon SageMaker, outline the process for evaluating data, explain why data needs to be pre-processed. Use open source tools to examine and pre-process data. Use Amazon SageMaker to train and host a machine learning model. Use cross validation to test the performance of a machine learning model. Use a hosted model for inference. And finally, create an Amazon SageMaker hyperparameter tuning job to optimize a model's effectiveness. We're ready to get started. See you in the next video.  "
Mod03_Sect02_part3.mp4,"Hi, welcome back. We'll continue exploring data collection by reviewing how to secure your data. It's important to consider the security of your data. Though the data sets used in this course are all public, real data about customer transactions or health records need to be kept secure. You can use AWS Identity and Access Management, which is also known as IAM. It's a service that controls access to resources. Make sure you're securing your data within AWS correctly so you can avoid data breaches. The diagram shows a simple IAM policy that allows only read access to a specific S3 bucket for the listed role. In addition to controlling access to data, you need to make sure your data is secure. It's a good practice and it might also be legally required for certain data types, such as financial data or health care records. AWS provides encryption features for storage services, typically for data that's at rest or in transit. You can often meet these encryption requirements by enabling encryption on the object or service you want to protect. For data in transit, you must use secure transports, like secure sockets layer, transport layer security or SSL TLS. Another aspect to consider is compliance audits. When dealing with data from regulated industries, you'll often need to audit access to the data. AWS CloudTrail is a service that enables governance, compliance, operational auditing, and risk auditing of your AWS account. With CloudTrail, you can log, continuously monitor, and retain account activity related to actions across your entire AWS infrastructure. CloudTrail provides an event history of your AWS account activity, including actions taken through the AWS Management Console, AWS SDKs, command line tools, and other AWS services. This event history simplifies security analysis, resource change tracking, and troubleshooting. You can also use CloudTrail to detect unusual activity in your AWS accounts. All these features can help you simplify operational analysis and troubleshooting. Here are the key takeaways for this section. We looked at the first step in solving machine learning problems, obtaining the data required to train your machine learning model. We also reviewed how ETL can be used to obtain data from multiple sources. Services like AWS Glue can make it easy to obtain data from multiple data stores. Finally, make sure you understand your security requirements. These are based on both business need and any regulatory requirements. Also, make sure your data is secure. Only authorized users should be able to access your data, and it should be encrypted where possible. That's it for section two. We'll see you in the next video.  "
Mod02_Intro.mp4,"Hi and welcome to Module 2 of AWS Academy Machine Learning. In this module, we're going to introduce machine learning. We'll first look at the business problems that can be solved by machine learning. We'll then talk about terminology, process, tools, and some of the challenges you'll face. After completing this module, you should be able to recognize how machine learning and deep learning are part of artificial intelligence. Describe artificial intelligence and machine learning terminology. Identify how machine learning can be used to solve a business problem. Describe the machine learning process. List the tools available to data scientists. And identify when to use machine learning instead of traditional software development methods. You're now ready to get started with Section 1. See you in the next video.  "
Mod03_Sect07_part3.mp4,"Hi, welcome back. We'll continue exploring how to evaluate your model. Classification models are going to return a probability for the target. This is a value of the input belonging to the target class, and it will be between 0 and 1. To convert the value to a class, you need to determine the threshold to use. You might think it's 50%, but you could change it to be lower or higher to improve your results. As you've seen with sensitivity and specificity, there's a trade-off between correctly and incorrectly identifying classes. Changing the threshold can impact that outcome. We're going to take a look at how you can visualize this. A receiver operating characteristic graph is also known as an ROC graph. It summarizes all the confusion matrices that each threshold produced. To build one, you calculate and plot the sensitivity or true positive rate against the false positive rate on a graph for each threshold value. You can calculate the false positive rate by subtracting the specificity from one. After you plot those points, you can draw a line between them. The dotted black line from 0, 0 to 1, 1 means that the sensitivity or true positive rate is equal to the false positive rate. The point at 1, 1 means that you've correctly identified all the cats, but you've also incorrectly identified all the not cats. This is bad. Any point on this line means that the proportion of correctly classified samples is the same as the proportion of incorrectly classified samples. The point at 0, 0 represents that there are zero true positives and zero false positives. A model that has high sensitivity and low false positive rate is usually the goal, so it's considered to be better when the line between the threshold recordings is closer towards the top left corner. If you had the data from two models, you could plot out the ROC curve for each model and compare them. However, that can be tedious. There's another graph you can use for this, which we'll look at next. Another evaluation metric you can use is the area under the curve, receiver operator curve, which is also known as an AUC ROC. The AUC part is the area under the plotted line. When the AUC is higher, it means the model will be better at predicting cats as cats and not cats as not cats. You can use the AUC to quickly compare models with each other. With the four numbers from our confusion matrix, you can calculate the model's accuracy. This is also known as its score. You can do this by adding up the correct predictions and then dividing that number by the total number of predictions. Though accuracy is widely used metric for classification problems, it has limitations. This metric isn't as effective when there are a lot of true negative cases in your data set. Think about the cat not cat example. If most of your accuracy is based on true negatives, it says that your model is good at predicting what isn't a cat. In this case, you might not feel confident in your model's ability to predict cats after you roll it out into production. This leads to an example of why it's important to make sure that the metric you choose for model evaluation aligns to your business goal. Think about the credit card fraud example. In this case, using accuracy as your main metric probably isn't a good idea because you have a lot of true negatives. Your high true negative number might hide the fact that your model's ability to identify cases of fraud, that is to identify true positives isn't ideal. As a credit card company, it's probably unacceptable to have less than almost perfect performance identifying fraud cases. That would drive customers away, which would be the opposite of what you'd want to achieve from a business standpoint. This is why two other metrics are often used in these situations. The first one is precision, which essentially removes the negative predictions. Precision is the proportion of positive predictions that are actually correct. You can calculate it by taking the true positive and dividing it by true positive plus false positive. When the cost of false positives is high in your particular business situation, precision might be a good metric. Think about a classification model that identifies email messages as spam or not. In this case, you don't want your model to label an email message as spam and thus prevent your users from seeing that message when it's actually legitimate. Or consider an example of a model that needs to predict whether a patient has a terminal illness. In this case, using precision as your evaluation metric doesn't account for false negatives in your model. Here, for the model to be successful, it's crucial that it doesn't falsely predict the absence of illness in a patient who actually has that illness. Sensitivity would be a better metric to use for this situation. But it doesn't always need to be one or the other. The F1 score combines precision and sensitivity together. It gives you one number that quantifies the overall performance of a particular ML algorithm. You should consider using an F1 score when you have a class imbalance, but want to preserve the equality between precision and sensitivity. But what do you do if you're dealing with a regression problem? In that case, there are other common metrics you can use to evaluate your model, including the mean squared error. The mean squared error is frequently used. Its general purpose is the same as what you saw with classification metrics. You determine the prediction from the model, and you compare the difference between the prediction and the actual outcome. More specifically, you take the difference between the prediction and actual value, square that difference, and then sum up all the squared differences for all the observations. In SkyKit Learn, you can use the mean squared error function directly from the metrics library. There are other metrics you can use for linear models, such as R squared. So you've trained your model, performed a batch transformation on your test data, and calculated your metrics. Now what will you do? You'll use these metrics to help you tune the model. You could select a different set of features and train the model again. After you retrain the model, ask yourself, which was the better model? The metrics will help inform you. You could also use different data and retrain the model with the same features. Remember K-fold cross validation from earlier in this module? Finally, you could tune the parameters of the model itself, which is the subject of the next section. Here are key takeaways from this section of the module. To evaluate the model, you need to have data that the model hasn't seen. This could be either a holdout set, or you could use K-fold cross validation. Different machine learning models use different metrics. Classification can use the confusion matrix and the AUCRO-C that you can generate from it. Regression can use mean squared. That's it for section 7. See you in the next video.  "
Mod03_Sect08.mp4,"Hi, and welcome back to Module 3. This is Section 8. In this section, we're going to take a look at how you can tune the model's hyperparameters to improve model performance. Recall from an earlier module that hyperparameters can be thought of as the knobs that tune the machine learning algorithm to improve its performance. Now that we're looking more explicitly at tuning models, it's time to look more specifically at the different types of hyperparameters and how to perform hyperparameter optimization. There are a couple of different categories of hyperparameters. The first kind are model hyperparameters. They help define the model itself. As an example, consider a neural network for a computer vision problem. For this case, additional attributes of the architecture need to be defined, like filter size, pooling, and the stride or padding. The second kind are optimizer hyperparameters. They relate to how the model learns patterns based on data, and they're used for a neural network model. These types of hyperparameters include optimizers like gradient descent and stochastic gradient descent. They can also include optimizers that use momentum like atom, or that initialize the parameter weights with methods like Xavier initialization or he initialization. The third kind are data hyperparameters. They relate to the attributes of the data itself. These include attributes that define different data augmentation techniques, like cropping or resizing for image-related problems. They're often used when you don't have enough data or enough variation in your data. Tuning hyperparameters can be very labor intensive. Traditionally, this was done manually by someone who had domain experience related to the hyperparameter and the use case. This person would manually select the hyperparameters based on their intuition and experience. Then they would train the model and score it on the validation data. This process would be repeated over and over again until they achieved satisfactory results. This manual process isn't always the most thorough and efficient way of tuning hyperparameters. With SageMaker, you can perform automated hyperparameter tuning with Amazon SageMaker Automatic Model Tuning. It finds the best version of a model by running multiple training jobs on your data set by using the algorithm and hyperparameter ranges use specify. It then chooses the hyperparameter values that results in a model that performs the best as measured by a metric you choose. It uses Gaussian process regression to predict which hyperparameter values might be most effective at improving fit. It also uses Bayesian optimization to balance exploring the hyperparameter space and exploiting specific hyperparameter values when appropriate. And importantly, automatic model tuning can be used with built-in algorithms from SageMaker, pre-built deep learning frameworks, and bring your own algorithm containers. Suppose that you want to solve a binary classification problem on a fraud data set. Your goal is to maximize the area under the AUC curve metric of the algorithm by training a linear learner algorithm model. You don't know which values of the learning rate, beta one, beta two, and epochs you should use to train the best model. To find the best values for these hyperparameters, you can specify ranges of values that SageMaker hyperparameter tuning will then search. It will find the combination of values that results in the training job that performs the best as measured by the objective metric that you chose. In the example, SageMaker hyperparameter tuning launches training jobs that use hyperparameter values in the ranges you specified, and then returns the training job with the highest AUC. Hyperparameter tuning might not necessarily improve your model. It's an advanced tool for building machine solutions. As such, it should be considered part of the scientific method process. When you build complex machine learning systems, like deep learning neural networks, exploring all possible combinations is impractical. To improve optimization, use the following guidelines when you create hyperparameters. First, instead of using all hyperparameters, limit the number of hyperparameters to the ones you think would give you good results. The range of values for the hyperparameters you choose to search can significantly affect the success of hyperparameter optimization. Although you might want to specify a large range that covers every possible value for a hyperparameter, you'll get better results by limiting your search to a small range of values. If you get the best metric values within a part of a range, consider limiting the range to only that part. During hyperparameter tuning, SageMaker attempts to figure out if your hyperparameters are log scaled or linear scaled. Initially, it assumes the hyperparameters are linear scaled. If they should be log scaled, it might take time for SageMaker to discover that on its own. If you know that a hyperparameter should be log scaled and you can convert it yourself, doing so can improve hyperparameter optimization. Running more hyperparameter tuning jobs concurrently gets more work done quickly, but a tuning job improves only through successive rounds of experiments. Typically, running one training job at a time achieves the best results with the least amount of compute time. Save it you have a distributed training job that runs on multiple instances. In this case, hyperparameter tuning uses the last reported objective metric from all instances of that training job as the value of the objective metric for that training job. Having distributed training jobs so that they report the objective metric you want. Now that you've gone through the end-to-end process of training and tuning a machine learning model, it's worth talking about Amazon SageMaker Autopilot. This service can help you find a good model with little effort or input on your part. With Autopilot, you create a job that supplies the test, training, and target. It will analyze the data, select appropriate features, and then train and tune the models. It will document the metrics and find the best model based on the provided data. The results include the winning model and metrics and a Jupyter Notebook you can use to investigate the results. Although using Autopilot doesn't remove your need to pre-process the data, it can save you time during feature selection and model tuning. Some key takeaways from this section of the module include these points. First, model tuning is important for finding the best solution to your business problem. Hyperparameters can be tuned for the model, optimizer, and data. SageMaker can perform automatic hyperparameter tuning. And finally, overall model development can be accelerated by using Autopilot. That's it for this video, see you in the next one.  "
Mod04_Sect02_part3.mp4,"Hi, and welcome back. In this section, we'll look at how you can use Amazon Forecast to create a predictor and generate forecasts. When you generate forecasts, you can apply the machine learning development pipeline you've seen throughout this course, but you still need data. You need to import as much data as you have, both historical data and related data. You want to do some basic evaluation and feature engineering before you use the data to train a model so you can meet the requirements of Amazon Forecast. To train a predictor, you need to choose an algorithm. If you're not sure which algorithm is the best for your data, Amazon Forecast can choose for you. To do this, select AutoML as your algorithm. You also need to select a domain for your data. If you're not sure what the best fit is, you can also select a custom domain. Domains have specific types of data they require. When you have a trained model, you can then use the model to make a forecast using an input dataset group. After you've generated a forecast, you can query the forecast. You can also export it to a bucket in Amazon S3. And finally, you can encrypt the data in the forecast before exporting it. The overall process for working with Amazon Forecast is to import historical and related data. Amazon Forecast inspects the data, identifies key data, and selects an appropriate algorithm. It uses the algorithm to train and optimize a custom model and produce a predictor. You create forecasts by applying the predictor to your dataset. You can then retrieve these forecasts in the AWS Management Console, or you can export the forecasts as comma-delimited files. You can also use an API and AWS CLI commands to create and retrieve forecasts. When you work with Amazon Forecast, you can select the domain you're working in. There are domains ranging from retail to web traffic, and there's also a custom option for everything else. By selecting a domain, you improve the efficiency of the predictor. Each domain has specific types of data that you'll supply when you build the predictor. For example, the retail domain expects data for the item identifiers, a timestamp for the observation, the number of sales for that item, and the specified timestamp. Here's an example of the data you'd need to provide for a retail demand forecast. For the time series, you need the time when the transaction took place, ideally in UTC format, the item ID of the item, and how many items were sold. The metadata for the item might include the category, the item color, and other attributes. The link back to the time series data will be only the item ID because item metadata typically doesn't change. Related data for creating a more useful forecast could include the sales price or other promotion data. To link this back to the item, you must include the timestamp and the item ID. Here's an example of the data you'd need to provide for a web traffic forecast. For the time series, you need the web page ID, the number of page views per month, and the timestamp. Related data for creating a more useful forecast could include the page category, such as navigation or content category, you'll also need the geographic identifier for the web plan. For metadata, you might also need to provide the region and the sales promotion information. Amazon forecast predictors use an algorithm to train a model. They then use the model to make a forecast using an input data set group. To help you get started, Amazon forecast provides predefined algorithms, Arima, Deep AR Plus, ETS, NPTS, and Profit. You can also use the AutoML feature. It will try all the algorithms to see which ones are the best at predicting data. When you prepare data for training and machine learning, you typically hold back data to use when you validate and score the model. The data that you hold back is usually a random sample of your available data. With time series data, you must process your data differently because of a correlation between time. When you import your data, Amazon forecast breaks it into training and test data sets, which the diagram shows. The training data is used to train the model, which is then tested against the data that was held back. You can specify multiple back test windows, which will split the data multiple times, train the model, and use metrics to determine which model gives the best results. The default back test window is one. You can change how Amazon forecast splits the data by setting the back test window offset parameter when you create the predictor. If you don't set this value, the algorithms use default values. After you've trained a model, you will need to measure its accuracy, which you'll learn about next. The first Amazon forecast evaluation metric is the weighted quantile loss or W quantile loss. When Amazon forecast creates a forecast, it provides probabilistic predictions at three distinct quantiles, 10%, 50%, and 90%. These prediction quantiles show you how much uncertainty is associated with each forecast. A P10 quantile predicts that 10% of the time the true value will be less than the predicted value. For example, suppose that you are a retailer, you want a forecast product demand for winter gloves that sell well only during the fall and winter. Say that you don't have sufficient storage space and the cost of invested capital is high or that the price of being overstocked on winter gloves concerns you. Then you might use the P10 quantile to order a relatively low number of winter gloves. You know that the P10 forecast overestimates the demand for your winter gloves only 10% of the time, so you'll be sold out of your winter gloves for 90% of the time. A P50 quantile predicts that 50% of the time the true value will be less than the predicted value. Continuing the winter gloves example, say you know that there will be a moderate amount of demand for the gloves and you aren't concerned about being overstocked. Then you might choose to use the P50 quantile to order gloves. A P90 quantile predicts that 90% of the time the true value will be less than the predicted value. Suppose you determine that being understocked on gloves will result in large amounts of lost revenue. For example, the cost of not selling gloves is extremely high or the cost of invested capital is low. In this case, you might choose to use the P90 quantile to order gloves. Amazon forecast also calculates the associated loss or error at each quantile. Weighted quantile loss calculates how far off the forecast a certain quantile is from actual demand in either direction. Lower W quantile loss metrics mean that the models forecast are more reliable. The root mean square error or RMSC is another method for evaluating the reliability of your forecasts. Like W quantile loss, RMSC calculates how far off the forecasted values were from the actual test data. The RMSC finds the difference between the actual target value in the data set and the forecasted value for that time period. And it then squares the differences. The example shows how to calculate RMSC. The RMSC value represents the standard deviation of the prediction errors. This test is good for forecast validity when the errors are mostly of the same size. That is, there aren't many outliers. Lower RMSC metrics indicate that the models forecasts are more reliable. Here's an example of how a web retailer might use the accuracy metrics to evaluate a forecast. The retailer wants to predict the demand for sales of a particular brand of shoes. They input the sales records for this brand into Amazon forecast to create a predictor. The predictor provides a forecasted demand of 1000 pairs with the P10, P50 and P90 values shown. The weighted quantile loss values indicate that 10% of the time there will be fewer than 880 pairs sold. 50% of the time, fewer than 1000 and 50 pairs will be sold. And 90% of the time, fewer than 1200 pairs will be sold. The retailer can then use these values to determine which level of inventory to hold. They can base their decision on their assessment of the risk that they won't be able to fulfill orders, or that they'll have excess inventory. Some key takeaways from the section of the module include, you can use Amazon forecast to train and use a model for time series data. There are specific schemas defined for domains such as retail and EC2 capacity planning, or you can use a custom schema. You need to supply at least the time series data, but can also provide metadata and related data to add more information to the model. As with most supervised machine learning problems, your data is split into training and testing data, but takes into account the time element. Use RMSE and W Quantile loss metrics to evaluate the efficiency of the model. That's it for this video. We'll see you in the next one.  "
Mod05_Sect03_part1.mp4,"In this section, we'll look at preparing custom data sets for computer vision so you can detect custom objects. One challenge of using a pre-built model is that it will only find images it was trained to find. Though Amazon recognition was trained with tens of millions of images, it can't detect objects that it wasn't trained on. For example, consider the 8 of hearts playing card. If you run this card through Amazon recognition, the results show various attributes. However, none of the labels are playing card or 8 of hearts. If you want Amazon recognition to detect images in your problem domain, you must train the model with your images. So in this section, you'll learn how to train Amazon recognition with images from your problem domain. Though you'll focus only on using Amazon recognition here, you'll encounter a similar process if you use other pre-trained models. Training a computer vision algorithm to recognize images requires a large input data set, which isn't practical for most organizations. Many machine learning problems today can be solved by training existing models, or you can use a managed service like Amazon recognition custom labels. Like other machine learning processes, you need to train Amazon recognition so it recognizes scenes and objects that are in a specific domain. You'll need both a training data set and a test data set that contain labeled images. If you have images that need labels, you can use Amazon recognition custom labels to simplify your labeling tasks. For example, it provides a UI for labeling images, which includes a feature you can use to draw bounding boxes around images. It can also help find objects and scenes that are unique to your business needs. You can use it to classify images or detect objects within an image. Say you want to identify specific machine parts in images, such as turbochargers or torque converters. You could collect pictures of each kind of machine part and use them to train your model. Amazon recognition custom labels also includes automated machine learning capabilities that handle the machine learning process for you. When you provide training images, the service can automatically load and inspect the data. Select the correct machine learning algorithms, train a model, and provide model performance metrics. When you finish training your model, you can then evaluate your custom model's performance on your test set. Each image in the test set has a side-by-side comparison of the model's prediction versus the label it assigned. There are also detailed performance metrics for you to review. You can start using your model immediately for image analysis, or you can iterate and retrain new versions with more images to refine the model. After you start using your model, you can track your predictions, correct any mistakes, and use the feedback data to retrain new model versions and improve their performance. So how do you label images? The diagram shows a typical process for training a computer vision model, which includes the Amazon recognition custom labels feature. We'll step through this in some detail. The process of developing a custom model to analyze images requires time, expertise, and resources. It often takes months to complete. It can also require thousands, or tens of thousands, of hand-labeled images so the model has enough data to make accurate decisions. It can take months to generate and gather this data, and it can require large teams of lablers to prepare it for use in machine learning. Amazon recognition custom labels builds on the existing capabilities of Amazon recognition, which is already trained on tens of millions of images across many categories. Instead of thousands of images, you can upload a small set of training images that are specific to your use case. Typically, you'd use a few hundred images for this. You can use the AWS Management Console to upload training images. If your images are already labeled, Amazon recognition custom labels can begin training your model. If they're not, you can label the images directly in the labeling interface, or you can use Amazon SageMaker Ground Truth to label them for you. They'll be more on that shortly. Amazon recognition custom labels works best when you use different models for different domains. For example, if you need to detect both machine parts and plant health, you'd use two different models. If you use use select for training, should be similar to the images that will be used for inference. Use images that use various lighting conditions, backgrounds, and resolutions. Ideally, your training images will mirror images you'd want to perform detection on. If you can use the same source, like you'd use in production, that works best. The documentation includes additional guidelines on image type, so whether they are JPEGs or PNGs and other properties, like image size and resolution. That's it for part one of this section. We'll see you again for part two, where we'll review how to create the training data set.  "
Mod03_Sect03_part3.mp4,"Hi, welcome back. Now we'll review how to find correlations in your data set. How can you quantify the linear relationship among the variables you're seeing in a scatter plot? A correlation matrix is a good tool in this situation. It conveys both the strong and weak linear relationships among numerical variables. One can go as high as one or as low as minus one. When the correlation is one, this means those two numerical features are perfectly correlated with each other. It's like saying why is proportional to x? When the correlation of those two variables is minus one, it's like saying why is proportional to minus x? Any linear relationship in between can be quantified by the correlation. So if the correlation is zero, this means there's no linear relationship. But it doesn't mean that there's no relationship. It's just an indication that there's no linear relationship between those two variables. However, looking at a number isn't always straightforward. Often it's easier to view the numbers when they're represented by colors. Now we'll look at the heat map. The highest number, one, in dark green, and minus one is in dark brown. The color gives you both the positive and negative directions. And it also shows how strong the correlations are. We can use the seaborne heat map function to show the correlation matrix. Looking at the chart, there's some correlation between citric acid and fixed acidity. That would be expected in line because citric acid contributes to the acidity of the wine. However, there isn't much correlation between fixed acidity and pH. pH is a measurement of the strength of those acids present. But fixed acidity is a measure of the quantity. In this particular data set, there doesn't appear to be a correlation here. Some key takeaways from this section of the module include these points. The first step is to get your data into a format that can be used easily. Pandas is a popular Python library for working with data. Descriptive statistics will help you gain insights into the data. You can use visualizations to examine the data set in more detail. That's it for this section. We'll see you again in the next video.  "
Mod02_Sect04.mp4,"Welcome back. In this section, we'll look at some of the tools you'll be using throughout the rest of this course. Before we start, this list isn't an exhaustive list of all the tools available today. We're only going to cover them at a high level, but it's a good place to get started. First, there's the Jupyter Notebook. The Jupyter Notebook is an open source web application you can use to create and share documents that contain live code, equations, visualizations, and narrative text. Uses include data cleaning and transformation, numerical simulation, statistical modeling, data visualization, machine learning, and much more. Jupyter Lab is a web-based interactive development environment for Jupyter Notebooks, code, and data. Jupyter Lab is flexible. You can use it to configure and arrange the user interface to support a wide range of workflows in data science, scientific computing, and machine learning. Jupyter Lab is extensible and modular. You can write plugins that add new components and integrate with existing ones. Later in this course, you'll use Amazon SageMaker, which hosts both Jupyter Notebooks and Jupyter Lab. Pandas is an open source Python library. It's used for data handling and analysis. Pandas represents data in a table similar to a spreadsheet. This table is known as a Pandas DataFrame. Matplotlib is a Python library for creating scientific, static, animated, and interactive visualizations in Python. You'll use it to generate plots of your data later in this course. Seaborn is another data visualization library for Python that's built on Matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphs. NumPy is one of the fundamental scientific computing packages in Python. It contains functions for n-dimensional array objects. It also has useful math functions such as linear algebra, Fourier transform, and random number capabilities. SypHitLearn is an open source machine learning library that supports supervised and unsupervised learning. It also provides various tools for model fitting, data preprocessing, model selection and evaluation, and many other utilities. SypHitLearn is built on NumPy, SyPy, and Matplotlib. It's a good tool for exploring machine learning. Although you'll only use it to borrow a few functions in this course, you might want to consider exploring it after you complete this course. Moving up from individual libraries and packages, there are also tools that contain production, ready frameworks. We already mentioned SypHitLearn, which is a good library for machine learning. The framework supported on AWS, such as TensorFlow and Keras, also include libraries you can use for machine learning. All the frameworks listed here are supported on AWS and can be used from Amazon SageMaker. AWS also provides compute instances that are tuned for machine learning in both the cloud and at the edge. Compute instances can be optimized for learning and inference. Another AWS resource you can use are certain Amazon machine images or AMIs. They offer pre-packaged AMIs that contain many of the popular frameworks. Finally, there's Amazon SageMaker, which is an AWS service with many capabilities. First, SageMaker can deploy machine learning instances running Jupyter notebooks and Jupyter lab. It manages the deployment of these compute resources, so you only need to connect to the Jupyter environment. SageMaker also provides tools for labeling data, training models, and hosting trained models. AWS Marketplace also provides a selection of ready-to-use model packages and algorithms from third-party machine learning developers. AWS also provides a set of managed machine learning services, and you can integrate them into your applications even if you don't have substantial machine learning experience. For computer vision, Amazon recognition provides object and facial recognition for both image and video. Also, Amazon Textract can extract text from images. Speech services include Amazon Poly, which can speak text. Another speech service is Amazon Transcribe, which converts spoken audio to text. For language, Amazon Comprehend uses NLP to find insights and relationships in text. Also, Amazon Translate can translate text into different languages. If you want to work with chatbots, Amazon Lex helps you build interactive conversational applications that use voice or text. For forecasting, Amazon Forecast uses machine learning to combine time series data with additional variables so you can build forecasts. And finally, if you'd like to work with recommendations, Amazon Personalize can help you create individual personalized recommendations for customers. These managed services have already been trained in many aspects of the problem domain. You only need to provide your specific data to get started. We're going to look at many of these managed services in the second half of this course, after you learn how to do things on your own. The key takeaways for this section include these points. First, Python is the most popular language for performing machine learning tasks. Jupiter notebooks provide you with a web-based hosted development environment for machine learning. You'll use Jupiter notebooks frequently in machine learning. There are a large number of open-source tools such as pandas that you'll use often as a machine learning practitioner. Finally, depending upon your requirements, you might start with low-level frameworks to create your own solution. You might also use tools such as Amazon SageMaker to help with some of the heavy lifting. Or you can simply use and adapt one of the managed Amazon ML services for your specific problem domain. That's it for this video. We'll see you in the next one.  "
Mod04_Sect01.mp4,"Hi, and welcome to Section 1. We'll get started by reviewing what forecasting is and some use cases for it. Forecasting is an important area of machine learning. It's important because there are so many opportunities for predicting future outcomes based on historical data. Many of these opportunities involve a time component. However, while the time component adds additional information, it also makes time series problems more difficult to handle compared to other types of predictions. You can think of time series data as falling into two broad categories. The first type is univariate data, which means there's just one variable. The second one is multivariate data, which means there's more than one variable. There are several common patterns in time series data. The first pattern is a trend. With a trend, you get a pattern with the values increasing, decreasing, or staying the same over time. There are seasonal patterns. These reflect times of the year, month, day, or other patterns. Cyclical patterns are similar to seasonal patterns. These are patterns that repeat. It's like a large retail sale event that happens the same time each year. Finally, there are changes in the data over time that appear to be random or that have no discernible pattern. There are many uses for forecasting. You can use forecasting and marketing applications, such as for sales forecasting or demand projections. It could also be used in inventory management systems that anticipate required inventory levels. Forecasting energy consumption can help predict when and where energy is needed. Weather forecasting systems can be used for governments and commercial applications, such as agriculture. That's it for this section. See you in the next video.  "
Mod03_Sect04_part1.mp4,"Hi and welcome to section 4. In this section, we're going to look at feature engineering. Feature engineering is one of the most impactful things you can do to improve your machine learning model. We'll now look at what it is. There are two things that can help make your model more successful. The first is feature selection. And the second is feature extraction or the process of creating features. In feature selection, you select the most relevant features and discard the rest. You can apply feature selection to prevent redundancy or irrelevance in the existing features. You can also use it to limit the number of features to help prevent overfitting. feature extraction builds valuable information from raw data by reformatting, combining and transforming primary features into new ones. This process continues until it yields a new data set that can be consumed by the model to achieve your goals. As the diagram shows, feature extraction covers a range of activities, from dealing with missing data to converting text data into numerical data. Although the list isn't exhaustive, it should give you some idea of the data handling that's needed to get data into a useful state. Many of the tasks are no different than any other job working with data. You'll want to make sure data is in the correct format, that it's consistently represented, correctly spelled, among other tasks. For example, you might combine data or extract data into multiple columns. Or you could also remove columns altogether. Specific to machine learning, you'll need to convert text columns to numerical values. You'll also need to decide how to handle outliers and potentially rescale your data. Next, we'll look at some of the more common tasks in this section. Most machine learning algorithms work best with numerical data. You'll need to make sure that all columns in your data set contain numeric data by converting or encoding it. You might need to make several passes through the data sheet before you can encode it. For example, you might have variability in the text values, such as rows that contain both medium and MED as values. If the categorical data has order to it, you'll want to encode the text into numerical values that capture this ordinal relationship. Say you have data showing maintenance costs. You might encode low to 1, medium to 2, high to 3, and very high to 4. After you've made sure your categorical data is all uniform, you can use tools like SkyKit Learn and Pandas to encode your data. If the categorical data doesn't have any order to it, then you'll need to break the data into multiple columns. This will help make sure you don't introduce an ordinal relationship to the data that isn't there. For example, suppose you assigned a value of 1 to the first color, such as red, and you then assigned 2 to the next value, say blue. The model could interpret blue as being more important than red because blue has a higher numeric value. Encoding non-ordinal data into multiple columns or features is a better way. Think of the new features like a checkbox. Consider the example. There are three features that were generated. The value 1 indicates that the instance has that feature like its color. That's it for this section. We'll see you again in the next video.  "
Mod03_Sect03_part2.mp4,"Hi, welcome back. We'll continue exploring how to describe your data. Now that your data is in a readable format, you can perform descriptive statistics on the data to better understand it. Descriptive statistics help you gain valuable insights into your data so that you can effectively pre-process the data and prepare it for your ML model. You look at how you can do that and discuss why it's so important. First, descriptive statistics can be organized into a few different categories. Overall statistics include the number of rows and the number of columns in your data set. This information, which relates to the dimensions of your data, is very important. For example, it can indicate that you have too many features, which can lead to high dimensionality and poor model performance. Attribute statistics are another type of descriptive statistic, specifically for numeric attributes. They're used to get a better sense of the shape of your attributes. This includes properties like the mean, standard deviation, variance, and minimum and maximum values. If you need to look at relationships between more than one variable, you can consider multivariate statistics. They mostly relate to the correlations and relationships between your attributes. For cases when you have multiple variables or features, you might want to look at the correlations between them. It's important to identify correlations between attributes because a high correlation between two attributes can sometimes lead to poor model performance. When features are closely correlated and they're all used in the same model to predict the response variable, there could be problems. For example, the model loss might not converge to a minimum state. So be aware of highly correlated features in your data set. Mean and median are two different measures describing the extent that your data is clustered around some value or position. Mean can be a useful method for understanding your data when the data is symmetrical. However, if your data is skewed or contains outliers, then median tends to provide the better metric for understanding your data as it relates to central tendency. For instance, if you have outliers with large values, the mean can be skewed one way and it wouldn't serve as an accurate representation of where your values are truly centered. Median isn't affected by outliers in the same way. We'll talk more about outliers soon. These are available and they can be viewed on numerical data by using methods such as describe. There are also other methods to calculate the mean, median, and others. You can also view statistics on single or multiple columns. You can even group data by specific values. For categorical attributes, you can look at the frequency of attribute values in your data set. That information will give you some idea about what is inside that categorical variable. The diagram here shows the car data set, which is made up of several categorical values. Buying, Maint, Log Boot, Safety and Class. Safety can be either low, medium, or high. From the describe function, you can see that there are three unique values, with low being the most frequent. Looking at the class column, it appears that the top value of the four is UNACC, which stands for Unaccounted. This accounts for 1,210 of the 1,728 values, or 70%. This might suggest an imbalance. For a target variable that's also of a categorical type, you can look at the class distribution to see whether there's a class imbalance in your data set. Inbalance data can mark a disproportionate ratio for your classes. For instance, your data set is made up of credit card transactions, but only a tenth of a percent is labeled as fraud. In this case, your algorithm might not learn well enough to predict examples of credit card fraud. Visualization could help you gain insights into your data that you might not be aware of otherwise. A histogram is often a good visualization technique for seeing the overall behavior of a particular feature. With a histogram, you can answer questions like, is the feature data normally distributed? How many peaks are there in the data? Is there any skewness for that particular feature? When using histograms for your data visualization, values are binned. The taller peaks of the histogram indicate the most common values. For numerical features, you can use density plots and box plots. In addition to histograms, to get an idea of what's inside that particular feature. Like a histogram, these visualizations will help you answer questions like, what's the range of the data? The peak of the data? Are there any outliers? Are there any special features? Using these questions helps you understand your data better and can also help you decide if you need to do more specialized data pre-processing. A box plot is a method for graphically depicting groups of numerical data through their quartiles. When you have more than two numerical variables in a feature data set, you might want to look at their relationship. A scatter plot is a good way to identify any special relationships among those variables. In this case, the left diagram has sulfates and alcohol. There are two numerical variables. Suppose you want to show the relationship between these variables. You can use a scatter plot to help you visualize that. There are plots scattered around, and the correlation among them might not be that high because the data is scattered. However, you might find some relatively positive relationships between the two variables. A scatter plot matrices help you look at the relationship between multiple different features. In pandas, you can easily create scatter plot matrices based on the columns you want to look at. This example has three columns, and it will give the pair-wise scatter plot for any two columns. With a scatter plot, you might want to identify special regions that a particular subset of data could fit into. In the example, is there a relationship between alcohol sulfates and quality? You could plot those values against good and poor quality wines like the example. Plotting gives you an idea of how useful particular variables can be if you're using them for a classification problem. That's it for part two of this section. We'll see you again for part three where we'll review correlations and the takeaways for this section.  "
Mod03_Sect02_part2.mp4,"Hi, welcome back. We'll continue exploring data collection by reviewing how to extract, transform, and load data. Data is typically spread across many different systems and data providers. This presents a challenge. You'll need to bring all these data sources together into something that can be consumed by a machine learning model. You can do this through extract, transform, and load, which is also known as ETL. The steps in ETL are defined this way. In the extract step, you pull the data from the sources to a single location. During extraction, you might need to modify the data, combine matching records, or do other tasks that transform the data. Finally, in the load step, the data is loaded into a repository, such as Amazon S3. A typical ETL framework has several components. As an example, consider the diagram. First, the Crawler A program connects to a data store, which can be a source or a target. It progresses through a ranked list of classifiers to determine the schema for your data. Then, it creates metadata tables in the AWS Glue Data Catalog. A job defines the business logic that's needed to perform ETL work. To run the job, you'll need to use a schedule or event. As a final note, the services we just discussed exist in the transform partition of the ETL process. AWS Glue is a fully managed ETL service that makes it simple and cost-effective to categorize your data, clean it, enrich it, and move it reliably between various data stores. AWS Glue consists of a central metadata repository known as the AWS Glue Data Catalog. This is an ETL engine that automatically generates Python or Scala code. It also provides a flexible scheduler that handles dependency resolution, job monitoring, and retries. AWS Glue is serverless, so you don't need to set up or manage any infrastructure. You can use the AWS Glue Console to discover data, transform it, and make it available for searching queries. The console calls the underlying services to orchestrate the work needed to transform your data. You can also use the AWS Glue API operations to interface with the AWS Glue Services. This way, you can edit, debug, and test your Python or Scala Apache Spark ETL code using a familiar development environment. AWS Glue is well suited to machine learning because it can receive label data that can be used for training. Here's an example. Say that you provide AWS Glue with training data that teaches the model what duplicate records in the data source look like. Then, AWS Glue can identify the duplicates and present them for further analysis by a data engineer. AWS Glue enables the orchestration of complex ETL jobs. In the example, AWS Glue crawls the data sources and presents the information to clients as a data catalog. AWS Glue can run your ETL jobs based on an event, such as getting a new data set. For example, you can use an AWS Lambda function to trigger your ETL jobs to run as soon as new data becomes available in Amazon S3. You can also register this new data set in the AWS Glue Data catalog as part of your ETL jobs. Although managed tools are available in AWS to manipulate data, a data scientist will also write scripts in their Jupyter Notebook to handle data. A very simple extract and load script is shown here. The imports and variables section imports the libraries that are used. Note that BOTO3 is the library for AWS. Variables are also set here for these zip files web location and a local folder for extraction. The Download and Extract section makes a web request saving the bytes from the URL as a stream. This stream is passed to the zip file function, which is then used to extract the data. With the Extracted files in a folder, the Upload to S3 section enumerates the folder's files and uploads each file to Amazon S3. If you discover that this script is used often, it should be migrated to a standalone function that can be imported by other Python applications. That's it for part two of this section. We'll see you again for part three, where we'll review how to secure your data.  "
Mod05_Intro.mp4,"Welcome back to AWS Academy Machine Learning. This is Module 5 and we have a great topic for you today, Computer Vision. In this module, we'll start with an overview of the Computer Vision space and you'll learn about some of the use cases and terminology. Next, we'll explore details about analyzing image and video with managed services from Amazon Web Services or AWS. Finally, we'll look at how you can use your own customized data sets for performing object detection. At the end of this module, you'll be able to describe the use cases for computer vision, describe the Amazon Managed Machine Learning Services available for image and video analysis. List the steps required to prepare a custom data set for object detection. Describe how Amazon SageMaker Ground Truth can be used to prepare a custom data set. And finally, use Amazon Recognition to perform facial detection. Thanks for watching. We'll see you in the next video.  "
Mod03_Sect04_part2.mp4,"Hi, welcome back. We'll continue exploring feature engineering by reviewing how to clean your data set. In addition to converting string data to numerical data, you'll need to clean your data set for several other potential problem areas. Before encoding the string data, make sure the strings are all consistent. You'll also need to make sure variables use a consistent scale. For example, if one variable describes the number of doors in a car, the scale will probably be between two and eight. But if another variable describes the number of cars of a particular type sold in the state of California, the scale will probably be in the thousands. Some data items might also capture more than one variable in a single value. For instance, suppose the data set includes variables that combine safety and maintenance into a single variable, such as safe high maintenance. You'll need to train your machine learning system for both variables, and also split that single variable into two separate variables. You might also encounter data sets that are missing data for some variables, and some data sets will include outliers. You'll cover techniques for dealing with these situations in this section. You might find that data is missing. For example, some columns in your data set could be missing data because of a data collection error, or maybe data wasn't collected on a particular feature until the data collection process was underway. Missing data can make it difficult to accurately interpret the relationship between the related feature and the target variable. So regardless of how the data ended up being missed, it's important for you to deal with this issue. Unfortunately, most machine learning algorithms can't handle missing values automatically. You'll need to use human intelligence to update missing values with data that's meaningful and relevant to the problem. Most Python libraries for data manipulation include functions for finding missing data. So how do you decide if you should drop or impute missing values? This question is answered in part by better understanding how those values came to be missing in the first place, and how much data the missing values represent within your larger data set. For instance, say the missing values are randomly spread throughout your data set, and don't represent a larger portion of its respective row or column. In this case, imputation is most likely the better option. In contrast, say that you have a column or row that has a large percentage of missing values. In this case, dropping the entire row or column would be preferred over imputation. If you decide to drop rows with missing data, you can use built-in functions to do this. For example, pandas dropNA function can drop all rows with missing data, or you can drop specific data values by using a subset. As an alternative to dropping missing values, you can impute values for those missing values. There are different ways to impute a missing value. For categorical values, the missing value is usually replaced with the mean, the median, or the most frequent values. For numerical or continuous variables, the missing value is usually replaced with the mean or the median. You can impute a single row of missing data, which is known as univariate. You can also do this for multiple rows, which is known as multivariate. We'll now look at a univariate example. Here, the sci-kit-learn imputer function is being used to impute some missing values. It's a fairly small data set, but there are two missing values. The missing value was imputed by the strategy of the mean. To do this, you first calculate the mean. Here it's the mean of 3 and 2, which is 2.5. Then you'll impute the mean value for the missing value. Some data libraries include an impute package that provides more complex ways to impute data. Examples include K nearest neighbor, soft impute, multiple imputation by chain equations, and others. That's it for part two of this section. We'll see you again for part three, where we'll review how to work with outliers in your data.  "
Mod03_Sect05.mp4,"Hi, welcome back to Module 3. This is Section 5 on Training. In this section, we're going to look at how to select a model and train it with the data we have pre-processed. At this point, you've done a lot to clean and prepare your data, but that doesn't mean your data is completely ready to train the algorithm. Some algorithms may not be able to work with training data in a data frame format. Some file formats like CSV are commonly used by various algorithms, but they do not make use of that optimizations that some of the file formats like Record I.O. Protobuff can use. Many Amazon SageMaker algorithms support training with data in a CSV format. And SageMaker requires that a CSV file doesn't have a header record and that the target variable is in the first column. Most Amazon SageMaker algorithms work best when you use the optimized Protobuff Record I.O. format for the training data. Using this format allows you to take advantage of pipe mode when training the algorithms that support it. Using pipe mode, your training job streams data directly from Amazon S3. When using the CSV format, the target variable in your training data set should be the first column on the left and your features should be to the right of the target variable column. Evaluating a model with the same data that it trained on will lead to overfitting. While overfitting is where your model learns the particulars of a data set too well, it's essentially memorizing the training data rather than learning the relationships between features and labels. This means the model isn't learning from those relationships and patterns to apply them to new data in the future. Hold out is when you split your data into multiple sets. Only sets for training data, validation data, and testing data. Training data, which includes both features and labels, feeds into the algorithm you've selected to produce your model. You then use the model to make predictions over the validation data set, which is where you'll likely notice things you'll want to tweak, and tune, and change. Then when you're ready, you run the test data set, which only includes features. Once you want the labels to actually be predicted, the performance you get here with the test data set is what you can reasonably expect to see in production. A common split when using the holdout method is using 80% of the data for a training set, 10% for validation, and 10% for test. Or if you have a lot of data, you can split it into 70% training, 15% validation, and 15% test. So for a small data set, we can use K-fold cross validation to utilize as much of the data as possible, while still having relatively good metrics in order to choose which model is better. K-fold cross validation randomly petitions the data into K different segments. For each segment, we'll use the rest of the data outside of it for training in order to do a validation on that particular segment. Let's look at an example. Here we have a five-fold cross validation. The available training data is separated into five different chunks. For the training of the first model, we're using all those chunks as the training data, and then we're going to calculate the metrics on this test piece. For the second model, we're going to use these pieces as training. Even the model is trained, you apply it to this test piece. We do the same thing five times. We use all the training data, and we test it on five different models on different chunks of the test data, eventually testing it on all data points. One other thing to note about splitting your data, data in a specific order can lead to biases on your model. This is especially true if you're working with structured data. For example, the wine data is ordered by the quality column. When you run your model against your test data, this ordered pattern will be applied by using the model. It might also mean that some targets are missing from the training data. Typically randomizing your data set prior to splitting is sufficient, and many libraries will provide functions for this. With smaller sets, it is sometimes useful to use stratified sampling. Stratified sampling ensures that the training and test sets have approximately the same percentage of samples of each target class as the complete set. An internet search will give you many ways to shuffle and split the data. One of the easiest is to use the Train Test Split function from SK Learn. Amazon SageMaker provides four different ways you can train models. The built-in algorithms available can be easily deployed from the AWS console, CLI, or a Jupyter Notebook. Containers are used behind the scenes when you use one of the Amazon SageMaker built-in algorithms, but you do not have to deal with them directly. Amazon SageMaker supported frameworks provide pre-built containers to support deep learning frameworks such as Apache MX Net, TensorFlow, PyTorch, and Chainer. It also supports machine learning libraries such as SkyKit Learn and Spark ML by providing pre-built Docker images. If you use the Amazon SageMaker Python SDK, they are deployed using their respective Amazon SageMaker SDK estimator class. If there is no pre-built Amazon SageMaker container image that you can use or modify for an advanced scenario, you can package your own script or algorithm to use with Amazon SageMaker. You can use any programming language or framework to develop your container. For an example, if your team works and builds ML models in R, you can build your own containers to train and host an algorithm in R as well. One else may have already developed and tuned a model. It is worth looking in the AWS marketplace to find available models. Amazon SageMaker provides high performance, scalable machine learning algorithms optimized for speed, scale, and accuracy. For supervised learning, Amazon SageMaker includes XG Boost and linear learner algorithms for classification and quantitative regression problems. There is also a factorization machine to address recommendation and time series prediction problems. Amazon SageMaker includes support for unsupervised learning such as with K means clustering and principal component analysis, PCA, to solve problems like identifying customer groupings based on purchasing behavior. Finally, there are a selection of specialized algorithms for processing images and other deep learning tasks. Let's look a little closer at three of the most commonly used built in algorithms and their use cases. XG Boost or Extreme Gradient Boosting is a popular and efficient open source implementation of the gradient boosted trees algorithm. The gradient boosting is a supervised learning algorithm that attempts to accurately predict a target variable by combining an ensemble of estimates from a set of simpler, weaker models. XG Boost has done remarkably well in machine learning competitions because it robustly handles a variety of data types, relationships, and distributions. The large number of hyperparameters can be tweaked and tuned for improved fit. This flexibility makes XG Boost a solid choice for problems in regression, classification, binary and multi-class, and ranking. The Amazon SageMaker linear learner algorithm provides a solution for both classification and regression problems. With the Amazon SageMaker algorithm, you can simultaneously explore different training objectives and choose the best solution from your validation set. You can also explore a large number of models and choose the best one for your needs. Compared with methods that provide a solution for only continuous objectives, the Amazon SageMaker linear learner algorithm provides a significant increase in speed over naive, hyperparameter optimization techniques. K-means is an unsupervised learning algorithm. It attempts to find discrete groupings within data where members of a group are as similar as possible to one another and as different as possible from members of other groups. You define the attributes that you want the algorithm to use to determine similarity. To train a model in Amazon SageMaker, you create a training job. The training job includes the URL of the Amazon S3 bucket where you stored the training data. The URL of the S3 bucket where you want to store the output of the job. The Amazon elastic container registry path where the training code is stored. The compute resources that you want Amazon SageMaker to use for model training. Compute resources are ML compute instances managed by Amazon SageMaker. Amazon SageMaker provides a selection of instance types optimized to fit different machine learning use cases. Instance types comprise varying combinations of CPU, GPU, memory, and networking capacity, and give you the flexibility to choose the appropriate mix of resources for building, training, and deploying your ML models. Each instance type includes one or more instance sizes, allowing you to scale your resources to the requirements of your target workload. Some key takeaways from this section of the module include split data into training and testing sets helps you validate the model's accuracy. K-fold cross validation can help with smaller data sets. Two key algorithms for supervised learning are XG Boost and Linear Learner. Use K-means for unsupervised learning. And use Amazon SageMaker to train models. That's it for section 5. I hope to see you in the next video.  "
Mod03_Sect04_part3.mp4,"Hi, welcome back. We'll continue exploring feature engineering by describing how to work with outliers. You might also need to clean your data based on any outliers that exist. Outliers are points in your data set that lie at an abnormal distance from other values. They're not always something you want to clean up because they can add richness to your data set. But they can also make it harder to make accurate predictions because they skew values away from the other more normal values related to that feature. An outlier might also indicate that the data point actually belongs to another column. You can think of outliers as falling into two broad categories. The first is a single variation for just a single variable or a univariate outlier. The second is a variation of two or more variables or a multivariate outlier. One of the more common ways to find univariate outliers is with a box plot. A box plot shows how far a data point is to the mean for that variable. The box in the plot shows the data values within two quartiles of the mean. Values outside that range are represented by the lines extending from the box which are sometimes called whiskers. A scatter plot can be an effective way to see multivariate outliers. For example, this diagram shows the amount of sulfates and alcohol in a collection of lines. With the scatter plot, you can quickly visualize whether there are multivariate outliers for the two variables. The origin of your outlier will most likely inform how you deal with it during this pre-processing phase of the pipeline or possibly later during feature engineering. There are several different approaches to dealing with outliers. You could delete the outlier if your outlier is based on an artificial error. This means the outlier isn't natural and was introduced because of some failure like incorrectly entered data. You could also transform the outlier by taking the natural log of a value. This in turn reduces the variation caused by the extreme outlier value which would then reduce the outliers influence on the overall data set. Finally, you could use the mean of the feature and impute that value to replace the outlier value. Again, this would be a good approach if the outlier was caused by artificial error. This isn't an exhaustive list, but it describes the most common options. Many of extracted features, you'll need to select the most appropriate features for training your model. There are three main feature selection methods. Filter methods use statistical methods to measure the relevance of features by their correlation with the target variable. Rapper methods measure how useful a subset of a feature is. They do this by training a model on the feature and then measuring how successful the model is. Filters are faster and cheaper than wrapper methods because they don't involve training the models repeatedly. Rappers typically find the best subset of features, but there's a risk of overfitting compared to using subsets of features from filter methods. Embedded methods are algorithm specific and they might use a combination of both filters and wrappers. Filter methods use a proxy measure instead of the actual model's performance. They're fast to compute, but they can still capture how useful the feature set is. Here are some common measures. The first is Pearson's correlation coefficient, which measures the statistical relationship or association between two continuous variables. The second is linear discriminant analysis or LDA. This is used to find a linear combination of features that separates two or more classes. The third is analysis of variance or ANOVA. This is used to analyze the differences among group means in a sample. And finally, chi-square is a single number that tells you how much difference exists between your observed counts and the counts you'd expect if there were absolutely no relationships in the population. Filters are usually less computationally intensive than wrappers, but they produce a feature set that isn't tuned to a specific type of predictive model. This lack of tuning means a feature set from a filter is more general than one from a wrapper. The filter also usually has a lower prediction performance than a wrapper. However, the filter's feature set doesn't contain the assumptions of a prediction model, so it's more useful for exposing relationships between features. Many filters provide feature ranking instead of an explicit best feature subset, and the cutoff point in the ranking is chosen through cross validation. Filters have also been used as a pre-processing step for wrappers, which enables a wrapper to be used on larger problems. Rapper methods use a predictive model to score feature subsets. Each new subset is used to train a model, which is then tested on a holdout set. The score for that subset is calculated by counting the number of mistakes made on that holdout set, or the error rate of the model. Because wrappers train a new model for each subset, they're computationally intensive. However, they usually provide the best performing feature set for that particular type of model or problem. The first selection starts with no features, and adds them until the best model is found. Backward selection starts with all features, drops them one at a time, and then selects the best model. Embedded methods combine the qualities of filter and wrapper methods. They're implemented by algorithms that have their own built-in feature selection methods. Some of the most popular examples of these methods are lasso and ridge regression. They have built-in penalization functions to reduce overfitting. Here are some key takeaways from this section of the module. First, feature engineering involves selecting the best features for machine learning. Pre-processing gives you better data to work with, and better data typically provides better results. Two categories for pre-processing are converting data to numerical values and cleaning up dirty data by removing missing data and cleaning outliers. Finally, how you handle dirty data impacts your model. That's it for section 4. We'll see you in the next video.  "
Mod03_Sect02_part1.mp4,"Hi, welcome back. We're now going to look at a few ways you can collect and secure data. In this section, we'll explore some of the techniques and challenges associated with collecting and securing the data that's needed for machine learning. Consider again the original example about predicting credit card fraud. You've further formulated the problem. What data do you need to actually train your model so you can get the desired output and subsequently achieve your intended business outcome? Do you have access to the data? If so, how much data do you have and where is it? What solution can you use to bring all this data into one centralized repository? The answer to these questions are essential at this stage. The good news for a budding data scientist is that there are many places where you can obtain data. Private data from you or your existing customer already exists, including everything from log files to customer invoice databases. Private data can be useful depending on the problem you're trying to solve. In many cases, private data is found in many different systems. We'll look at how to bring these sources together shortly. Sometimes, you want to use data that was collected and made available by a commercial organization. These such as Reuters, Change Healthcare, Dunn and Bradstreet, and ForSquare maintain databases you can subscribe to. They include curated news stories, anonymized healthcare transactions, global business records, and location data. If you supplement your own data with commercial data, you can get useful insights you wouldn't have gotten otherwise. There are also many open source data sets ranging from wine quality to movie reviews. These data sets are made available for use in research or for teaching purposes. AWS, Kaggle, and the UCI machine learning repositories are good places to find open source data sets. Government and health organizations are other sources of data that could be useful. Most machine learning problems need a lot of data. These are also called observations, and you already need to know the target answer or prediction for that data. This kind of data, where you already know the target answer or prediction, is called labeled data. Each observation in your data is made up of two elements, the target and the features. The target is the answer you want to predict. So in the credit card transaction example, the target of any given observation is either fraud or not fraud. A feature is an attribute of the example that you can use to identify patterns for predicting the target answer. A feature in the credit card example could be the date of the transaction, the vendor, or the amount in dollars of the transaction. You might wonder if the source of the target is fraud or not fraud. Typically, this information is discovered only after the transaction is complete, and the actual card owner notices a fraudulent transaction on their statement. This information would be recorded with the transaction for exactly the purpose of using it to train a future model. So given what you know about the elements of an ML data set, we'll return to one of the original questions. What data do you need to actually train your model to reach the desired output, and subsequently your intended business outcome? This is an example of a stage in the ML pipeline when it's crucial to get domain expertise to help you answer this question. With domain knowledge, you can start determining the features and target data your model will need to make accurate predictions. Your data should be representative of the data you'll have when you're using the model to make a prediction. For example, if you want to predict credit card fraud, you need to collect data for positive or fraudulent transactions. You also need to collect data for negative or non-frogulent transactions. You need both types of data, so the machine learning algorithm can find patterns that will distinguish between the two types. Suppose your average amount of fraudulent transactions is actually 3%, but your training data set only includes a very small fraction of fraudulent observations, say 0.4%. In this case, it'll be difficult for your model to truly learn patterns related to fraudulent transactions that it might encounter in production. There are many different services in AWS where you could find or store your data. Here are some key services you might use. Amazon's simple storage service is also known as Amazon S3. It provides object level storage. With S3, you can store as much data as you want in the form of objects, which you can think of as files. They could be CSV files or files of other formats you need. S3 can be accessed through the Web-based AWS Management Console. You can also access S3 programmatically through the API and SDKs or with third-party solutions, which also use the API and SDKs. If your training data is already in S3 and you're planning to run training jobs several times with different algorithms and parameters, you could use Amazon FSX for Luster. It's a file system service that speeds up your training jobs by serving your S3 data to Amazon SageMaker at high speeds. The first time you run a training job, FSX for Luster automatically copies data from S3 and makes it available to SageMaker. You can use the same Amazon FSX file system for subsequent iterations of training jobs, which prevents repeated downloads of common S3 objects. Alternatively, your training data might already be in Amazon Elastic File System or Amazon EFS. If so, we recommend using EFS as your data source for training data. It can launch your training jobs directly from the service without needing data movement, which results in faster training start times. This is often the case in environments where data scientists have home directories in Amazon EFS. They can quickly iterate on their models by bringing in new data, sharing data with colleagues, and experimenting with different fields or labels in their data set. For example, a data scientist can use a Jupyter Notebook to do an initial cleansing on a training set and launch a training job from Amazon SageMaker. They could then use their Jupyter Notebook to drop a column and relaunch the training job and finally compare the resulting models to see which one works better. There are many other AWS services and resources where you might find data. For example, you could use Amazon Relational Database Service or Amazon RDS, a Manage Relational Database Service. You could also use Amazon Redshift, which is a Manage Data Warehouse Service. Another option is Amazon TimeStream, a Manage Time Series database designed specifically to handle large amounts of data from the Internet of Things or IoT. You could even spin up your own instances on Amazon Elastic Compute Cloud, which is also known as Amazon EC2, and post your own database on these instances. When you have data sources, you'll need to extract useful data from these sources when assembling your data for machine learning. We'll look at this next. That's it for part one of this section. We'll see you again for part two where we'll review how to extract, transform, and load data.  "
Mod04_Intro.mp4,"Hi and welcome to Module 4 of AWS Academy Machine Learning. In this module, we're going to look at forecasting. We'll start with an introduction to forecasting and look at how time series data is different from other kinds of data. Then, we're going to look at Amazon Forecast, a service that helps you simplify building forecasts. At the end of this module, you'll be able to describe the business problem solved with Amazon Forecast, describe the challenges of working with time series data, list the steps required to create a forecast by using Amazon Forecast, and use Amazon Forecast to make a prediction. See you in the next video.  "
Mod04_Sect02_part2.mp4,"Hi, welcome back. We'll continue exploring wrangling time series data. Seasonality in data is any kind of repeating observation where the frequency of the observation is stable. For example, in sales, you typically see higher sales at the end of a quarter and into the fourth quarter. Former retail sees even higher sales than the fourth quarter. Be aware that data can have multiple types of seasonality in the same data set. There are many times when you should incorporate seasonality information into your forecast. For instance, localized holidays are a good example for sales. The chart shows that the total revenue generated by arcades has a strong correlation with the number of computer science doctorates awarded in the US. But correlations do not mean causation. If you disagree, see the source for the chart. There are many other correlations plotted on the site and none of them make any sense. With your own data, be careful that you're not seeing and acting on correlations that don't have meaning in the real world. Here's an experiment. If you generate two random time series data sets of numbers between zero and one, you'll find that they have a very low correlation. But if you introduce the same slope to both data sets, you'll see a very strong correlation. You need to know how stable a system is. The level of stability or stationarity can inform how much you should expect the system's past behavior to inform future behavior. A system with low stability won't be successful at predicting the future. You'll often want to determine the trend for a time series. But if you adjust the series for the trend, it can be difficult to compare it with another series that was also adjusted for the trend. This is because the trends might dominate the values in the series. This could then lead to overestimates in correlation between the two series, like we discussed previously. Auto-correlation is one of the special problems you face with time series data. As you've seen in other machine learning problems, the goal of building an ML model is to make sure you're separating the signal from the noise. Auto-correlation is a form of noise because separate observations aren't independent of each other. A time series with auto-correlation might overstate the accuracy of the model that's produced. Some of the algorithms you'll look at in this module can help correct for auto-correlation. These factors, along with seasonality, will influence the model you'll select to produce your forecast. Some algorithms handle seasonality and auto-correlation, but others do not. Data-swas developed with financial data analysis in mind. As such, it's good at handling time series data. First, you can set the index for your PENTAGE data frame to be a date time. You can then use date and time to select your data. You can use ranges that contain partial dates. You can also extract date parts such as year, month, weekday name, and more. For grouping and resampling tasks, PENTAGE has built-in functions to do both. Finally, PENTAGE can give you insights into auto-correlation. For more information about PENTAGE and the time series, refer to the PENTAGE documentation. One of the tasks in building a forecasting application is to choose an appropriate algorithm. Your choice of algorithm should be determined by the type of data set you're using and the features of that data set. Amazon Forecast supports these five algorithms, but there are others. Each algorithm can handle data with slightly different characteristics. For example, take auto-regressive, integrated moving average, which is also known as ARIMA. It removes auto-correlations that can influence the pattern of observations. Or take exponential smoothing, which is also known as ETS. This algorithm is useful for data sets with seasonality. You can find out more about these algorithms in the Amazon Forecast documentation. Some key takeaways from this section of the module include time series data is sequence data that includes a time element which makes it different from regular data sets. Some of the time challenges include dealing with different time formats, handling missing data through down sampling, up sampling, and smoothing. Dealing with seasonality such as weekdays and yearly cycles, avoiding bad correlations. PENTAGE has excellent time series support with functions for dealing with time. There are five algorithms used by Amazon Forecast, ARIMA, DeepAR+, ETS, NPTS, and Profit. That's it for this section. We'll see you in the next video.  "
Mod03_Sect06.mp4,"Hi, and welcome back. This is Section 6, and we're going to look at hosting and using the model. In this section, we'll look at how you can deploy your trained model so it can be consumed by applications. After you've trained, tuned, and tested your model, you'll learn more about testing in the next section. You're now ready to deploy your model. If you're thinking that we're looking at the phases out of order, here's why we're discussing deployment now. If you want to test your model and get performance metrics from it, you first need to make an inference or prediction from the model, and this typically requires deployment. Deployment for testing is different from production, although the mechanics are the same. Amazon SageMaker provides everything you need to host your model for simple testing and evaluation, from a few requests to deployments handling tens of thousands of requests. There are two ways you can deploy your model. For single predictions, you can deploy your model with Amazon SageMaker hosting services. SageMaker will deploy multiple compute instances which run your model behind a load-balanced end point. Applications can call the API at the end point to make predictions. With this model, you can scale the number of instances up or down based on demand. To get predictions for an entire dataset, use Amazon SageMaker batch transform. Instead of deploying and maintaining a permanent end point, SageMaker will spin up your model and perform the predictions for the entire dataset you provide. It will then store the results in Amazon S3 before it shuts down and terminates the compute instances. It's useful for performing batch predictions when you test the model. You can quickly run your entire validation set against the model without writing any code to process and collate the individual results. The goal of the deployment phase is to provide a managed environment to host models for providing inference securely and with low latency. After your model is deployed into production, you should monitor your production data and retrain your model if necessary. Newly deployed models need to reflect the current production data. New data is accumulated over time and it could potentially identify alternative or new outcomes. And so deploying a model is not a one-time exercise. Instead, it's a continuous process. With one click, you can deploy your model on Amazon ML instances that can automatically scale across multiple availability zones for higher redundancy. Just specify the type of instance and the maximum and minimum number of instances desired. SageMaker will take care of the rest. It will launch the instances, deploy your model, and set up the secure HTTPS endpoint for your application. Your application only needs to include an API call to this endpoint to achieve inference with low latency and high throughput. With this architecture, you can integrate your new models into your application in minutes because changes to the model no longer need changes to the application code. SageMaker manages your production, compute infrastructure on your behalf. It can perform health checks, apply security patches, and conduct other routine maintenance all with built-in Amazon CloudWatch monitoring and logging. Whenever you've trained the model, you can create the endpoint either in code or by using the SageMaker console. If you're planning to host only a single model, you can create an endpoint for that model. But if you're planning to host multiple models, you need to create a multi-model endpoint. Multi-model endpoints provide a scalable and cost-effective solution for deploying large numbers of models. They use a shared serving container that's enabled to host multiple models. This reduces hosting cost by improving endpoint utilization compared to using single model endpoints. It also reduces deployment overhead because SageMaker manages loading models in memory and scaling the models based on the traffic patterns to them. When you deploy machine learning models into production to make predictions on new data, you need to make sure you apply the same data processing steps that were used in training to each inference request. Otherwise, you can get incorrect prediction results. By using inference pipelines, you can reuse the data processing steps from model training during inference without maintaining two separate copies of the same code. This helps ensure the accuracy of your predictions and reduces development overhead. As SageMaker is a managed service, inference pipelines are completely managed. When you deploy the pipeline model, the service installs and runs the sequence of containers on each EC2 instance in the endpoint or each batch transform job. Additionally, the sequence of feature processing and inference runs with low latency because the containers are collated on the same EC2 instances. Some key takeaways from this section of the module include these points. You can deploy your train model by using SageMaker to handle API calls from applications or to perform predictions using a batch transformation. The goal of your model is to generate predictions to answer the business problem. Be sure that your model can generate good results before you deploy to production. Finally, use multi-model endpoint support to save resources when you have multiple models to deploy. That's it for this section. We'll see you in the next video.  "
Mod03_Sect07_part2.mp4,"Hi, welcome back. We'll continue exploring how to evaluate your model. The diagram shows the confusion matrix of how two different models performed on the same data. Can you tell which one's better? Which is better? Isn't a good question to ask. What do you mean by better? Does better mean making sure you find all the cats? Even if it means you'll get many false positives? Or does better mean making sure the model is the most accurate? It's difficult to see just by looking at the two charts. What if you're trying several models using multiple folds and have hundreds of data points to compare? To do that, you'll need to calculate more metrics. The first metric is sensitivity. This is sometimes referred to as recall, hit rate, or true positive rate. Identity is the percentage of positive identifications. In the cat example, it represents what percentage of cats were correctly identified. To calculate sensitivity, take the number of true positives, or the number of positive identifications of cats, and divide that by the total number of actual cats. In this example, 60% of cats that were cats were correctly identified as cats. Specificity is sometimes referred to as selectivity or true negative rate. Specificity is the percentage of negatives correctly identified. In the cat example, this is the number of images that were not cats that were correctly identified as not cats. To calculate specificity, take the number of true negatives and divide that by the total number of actual negatives. So for the example, that's the number of not cats that were correctly identified, divided by the total number of actual not cats. This means that in the example, 64% of not cats were identified as not cats. Now that you have these metrics for each model, knowing what your business goal is, makes it easier to decide which model to use. Which model would you choose if you wanted to make sure you'll identify as many cats as possible? Model B would be a good answer if you're not concerned about having many false positives that is. If you're not concerned about having incorrectly identified not cats. Which model would you choose if you wanted to make sure you identified animals that were not cats? Model A might work for this scenario. Again, it would depend on how many false negatives you can tolerate. If this was a classification of patients who had heart disease or not, which model would be best? This is where it gets interesting. A fun website might get a bad reputation if it can't identify cats correctly. But if you're trying to diagnose patients, your focus will probably be very different. It's important to understand the trade-offs you're making when you decide which model to use. There are also other metrics that can help you make your decisions. That's it for part two of this section. We'll see you again for part three where we'll start looking at thresholds.  "
Mod02_Sect02.mp4,"Hi, and welcome back. In this section, we're going to look at the types of business problems machine learning can help you solve. Machine learning is used all across your digital lives. Your email spam filter is the result of a machine learning program that was trained with examples of spam and regular email messages. Based on books you're reading or products you bought, machine learning programs can predict other books or products you're likely to be interested in. Again, the machine learning program was trained with data from other reader's habits and purchases. When detecting credit card fraud, the machine learning program was trained on examples of transactions that turned out to be fraud, along with normal transactions. You can probably think of many more examples from social media applications using facial detection to group your photos, to detecting brain tumors in brain scans or finding anomalies in X-rays. There are three main types of machine learning. There's supervised learning where a model uses known inputs and outputs to generalize future outputs. There's unsupervised learning where the model doesn't know inputs or outputs, so it finds patterns in the data without help. There's reinforcement learning where the model interacts with its environment and learns to take actions that will maximize rewards. It's important to know the different types of ML because the type will guide you towards selecting algorithms that make sense for solving your business problem. Let's look more into each of these types. Supervised learning is a popular type of ML because it's widely applicable. It's called supervised learning because there needs to be a supervisor, a teacher who can show the right answers, so to speak. Like any student, a supervised algorithm needs to learn by example. Essentially, it needs a teacher who uses training data to help it determine the patterns and relationships between the inputs and outputs. If you want to build an application to detect credit card fraud, you'd need training data that includes examples of fraud and examples of normal transactions. Within supervised learning, there are different types of problems, classification and regression. There are two subtypes of classification problems. The first is binary classification. Think back to the example with identifying fraudulent transactions. The target variable in this example is limited to two options, fraudulent or not fraudulent. This is a binary classification problem. There are also multi-class classification problems. These ML problems classify an observation into one of three or more categories. Say that you have an ML model that predicts why a customer is calling your store so you can reduce the number of transfers needed before the customer gets to the correct customer support department. In this case, the different customer support departments represent the variety of potential target variables, which could be many different departments, much more than just two. There are also regression problems. In a regression problem, you're no longer mapping an input to a defined number of categories. Instead, you're mapping an input to a continuous value like an integer. One example of an ML regression problem is predicting the price of a company's stock. Computer vision is a good example of supervised learning. Is this a cat or a dog? Is there a tumor in this X-ray? Computer vision is often built with deep learning models. It automates the extraction, analysis, classification and understanding of useful information from a single image or a sequence of images. Computer vision enables machines to identify people, places, and things in images with accuracy at or above human levels and with greater speed and efficiency. The image data can take many forms, such as single images, video sequences, views from multiple cameras or three-dimensional data. You'll learn more about computer vision later in this course. We'll now discuss unsupervised machine learning. Sometimes all you have is the data. There's no supervisor in the room. In unsupervised learning, labels aren't provided like they are with supervised learning. You don't know all the variables and patterns. In these instances, the machine has to uncover and create the labels itself. These models use the data they're presented with to detect emerging properties of the entire data set. Then, they construct patterns from those properties. Clustering is a common subcategory of unsupervised learning. This kind of algorithm groups data into different clusters based on similar features. It does this to better understand the attributes of a specific cluster. For example, by analyzing customer purchasing habits, unsupervised algorithms can identify groups of customers that are associated with the size tier of a company. The advantage of unsupervised algorithms is that they enable you to see patterns in the data that you weren't aware of before. Visual language processing is also known as NLP. This is another area of machine learning that's experiencing growth. If you've ever used Alexa or any other voice assistant, they'll use NLP to try and answer your question. NLP isn't just about speech. It's also about written text. NLP shows up in many applications. For example, NLP is used with chat or call center bots, which are automated systems that help you get your bank balance or order food from a restaurant. You can use NLP and translation tools which convert text between languages. For example, you might use applications that translate menus in real time. NLP is also used in voice to text translations, which converts spoken words into text. And finally, NLP can be used in sentiment analysis, which you can use to analyze the sentiment of comments and reviews of products, music, and movies. These sentiments could be used to give the movie an audience rating. You'll learn more about NLP later in this course. Another kind of machine learning that's been gaining popularity recently is reinforcement learning. Unlike other machine learning, reinforcement learning continuously improves its model by mining feedback from previous iterations. In reinforcement learning, an agent continuously learns through trial and error as it interacts in an environment. Reinforcement learning is broadly useful when the reward of a desired outcome is known, but the path to achieving it isn't. And that path requires a lot of trial and error to discover. Take the example of AWS DeepRacer. In the AWS DeepRacer simulator, the agent is the virtual car. The environment is a virtual racetrack. The actions are throttle and steering inputs to the car. And the goal is completing the racetrack as quickly as possible without deviating from the track. The car needs to learn the desired driving behavior to reach the goal of completing the track. For the car to learn this, AWS DeepRacer teams use rewards to incentivize their model to learn the desired driving behavior. In reinforcement learning, the thing driving the learning is called the agent. In this case, it's the AWS DeepRacer car. The environment is the place where the agent learns, which in this example would be the marked racetrack. When the agent does something in the environment that provokes a response, such as crossing a boundary, it shouldn't cross. That's called an action. That response is called a reward or penalty, depending on whether the agent did something to be reinforced or discouraged in the model. As the agent moves within the environment, its actions should start receiving more rewards and fewer penalties until it meets the desired business outcome. Self-driving vehicles bring together many machine and deep learning algorithms and models to solve the problem of driving from point A to point B. Two of its main tasks are the continuous detection of the environment and forecasting changes. These involve detecting objects and localizing and predicting the movement of the detected objects. The outputs of these findings act as inputs to other systems that make decisions on what they should do with the vehicle's various controls. There are use cases in self-driving vehicles that require real-time responses to the environment. For example, if a previously hidden pedestrian walks out from behind an obstacle, the vehicle brakes need to be applied immediately. There can be no latency or room for error with these actions. Not every problem should be solved with machine learning. Sometimes regular programming will work well for your needs. If you're interested in exploring a potential machine learning solution, look for the existence of large data sets and a large number of variables. Machine learning is often the best choice if you're uncertain of the business logic or procedures needed to obtain an answer or accomplish a task. Machine learning systems can be complex. The supporting infrastructure, management support, and technical expertise need to be in place to help ensure the project's success. Here are the key takeaways for this section, where we explored some machine learning applications that are already part of everyday life. First, machine learning problems can be grouped into three categories. Supervised learning is where you have training data where you already know the answer. Unsupervised learning is where you have data but are looking for insights within the data. Reinforcement learning is where the model learns based on experience and feedback. Most business problems are supervised learning problems. Let's it for this section. We'll see you in the next video.  "
Mod07_Sect01.mp4,"Welcome to Module 7, course wrap up. Congratulations on completing the AWS Academy Machine Learning course. We'll take a few minutes to review what you've learned and where you can go from here. We're going to start with a review of what you've learned in this course. You learned how to describe machine learning, implement a machine learning pipeline, and use Amazon Machine Learning Services for forecasting, computer vision, and natural language processing. Well done. Although this course isn't designed to prepare you to become certified for the AWS Certified Machine Learning specialty, we'll review how you can continue to work towards that certification. AWS Certification helps you build credibility and confidence by validating your cloud expertise with an industry-recognized credential. It also helps organizations identify skilled professionals who can lead cloud initiatives by using AWS. You must earn a passing score by taking a proctored exam to earn an AWS certification. After receiving a passing score, you'll receive your certification credentials. AWS Certification doesn't publish a list of all services or features that are covered in a certification exam. However, there's an exam guide for each exam, and it lists the current topic areas and objectives covered in the exam. Exam guides can be found on the Prepare for your AWS Certification exam webpage. You'll be required to update your certification or recertify every three years. View the AWS Certification Re-certification page for more details. The information on this slide is current as of June 2020. However, exams are frequently updated. Also, the details regarding which exams are available and what topics are tested by each exam are subject to change. The AWS Certified Machine Learning specialty means you can select and justify the appropriate machine learning approach for a given business problem. You can also identify appropriate AWS services to implement machine learning solutions. And finally, you can design and implement scalable, cost-optimized, reliable, and secure machine learning solutions. Before sitting for the AWS Certified Machine Learning specialty exam, we recommend that you have the following knowledge and experience. First, you should have one to two years of experience developing, architecting, or running ML or deep learning workloads on the AWS Cloud. Your experience should include performing basic hyperparameter optimization and working with machine learning and deep learning frameworks. You should also be able to express the intuition behind basic ML algorithms. Finally, you should be able to follow best practices for model training in addition to best practices for deployment and operations. Thanks for watching and congratulations on completing the AWS Academy Machine Learning course.  "
Mod06_Sect02.mp4,"Welcome back. In this section, we'll review five managed machine learning services you can use for various use cases. These services simplify the process of creating a machine learning application. We'll start by looking at Amazon Transcribe. You can use Amazon Transcribe to recognize speech in audio files and produce a transcription. It can recognize specific voices in an audio file and you can create a customized vocabulary for terms that are specialized for a particular domain. You can also add a transcription service to your applications by integrating with web sockets, an internet protocol you can use for two-way communication between an application and Amazon Transcribe. Here are some of the more common use cases for Amazon Transcribe. First, medical professionals can record their notes and Amazon Transcribe can capture their spoken notes as text. Also, video production organizations can generate subtitles automatically from video. This could also be done in real time for a live feed to add closed captioning. Media companies can use Amazon Transcribe to capture and label content. They can then feed the content into Amazon Comprehend for further analysis. Last, companies can record customer service or sales calls and transcribe them. They can analyze the results for training or for strategic opportunities. Amazon Poly can convert text into lifelike speech. You can input either plain text files or a file that's formatted in speech-synthesis markup language or SSML. SSML is a markup language used to provide special instructions for how speech should sound. For example, if you want to introduce a pause in the flow of speech, you can add an SSML tag that instructs Amazon Poly to pause between two words. You can also output speech from Amazon Poly to MP3, Vorbis, and PCM audio stream formats. Amazon Poly is eligible for use with certain regulated workloads. For example, it's eligible for use with the U.S. Health Insurance Portability and Accountability Act of 1996 or HIPAA. Amazon Poly is also eligible for use with payment card industry data security standard or PCI DSS. Here are some of the more common use cases for Amazon Poly. As a first example, major news companies are using Amazon Poly to generate vocal content directly from the written stories. It's also been embedded in mapping APIs so developers can add voice to their geobase applications. Language training companies have used Amazon Poly to create systems for learning a new language. Finally, animators have used it to add voices to their characters. With Amazon Translate, you can create multi-language experiences in your applications. You can create systems that read documents in one language and then render or store them in another language. You can also use it as part of a document analysis system. Amazon Translate is fully integrated with other machine learning services, such as Amazon Comprehend, Amazon Transcribe and Amazon Poly. With this integration, you can extract named entities, sentiment and key phrases by integrating it with Amazon Comprehend, create multi-lingual subtitles with Amazon Transcribe, and speak translated content with Amazon Poly. Here are some of the more common use cases for Amazon Translate. The first use case is building international websites. You can use Amazon Translate to quickly globalize your websites. Amazon Translate can also be used to develop multi-lingual chatbots. Chatbots are used to create a more human-like interface to applications. With Amazon Translate, you can create a chatbot that speaks multiple languages. Another use case is software localization. Translation is a major cost for all software aimed at a global audience. Amazon Translate can decrease software development time and significantly reduce costs for localizing software. The final example use case is international media management. Companies that manage media for a global audience have used Amazon Translate to reduce their costs for localization. Amazon Comprehend implements many of the NLP techniques that we reviewed earlier in this module. You can extract key entities, perform sentiment analysis, and tag words with parts of speech. Here are some of the more common use cases for Amazon Comprehend. The first example is analyzing legal and medical documents. Legal, insurance, and medical organizations have used Amazon Comprehend to perform many of the NLP functions we reviewed in this module. Another use is for large-scale mobile app analysis. Mobile app developers use Amazon Comprehend to look for patterns of usage with their apps so they can design improvements. Financial Fraud Detection is another use case for Amazon Comprehend. Banking, financial, and other institutions have used it to examine very large data sets of financial transactions to uncover fraud and look for patterns of illegal transactions. Finally, it can be used for content management. Media and other content companies can use Amazon Comprehend to tag content for analysis and management. With Amazon Lex, you can add a human language front end to your applications. Amazon Lex lets you use the same conversational engine that powers Amazon Alexa. You can automatically increase capacity for your Amazon Lex solution by creating AWS Lambda functions at scale on demand. You can also store log files of the conversations for further analysis. Here are some of the more common use cases for Amazon Lex. The first use case is building front end interfaces for inventory management and sales. All three interfaces are becoming more common. Companies have used Amazon Lex to add chatbots to their inventory and sales applications. Another use for Amazon Lex is creating customer service interfaces. Human-like voice applications are quickly becoming the standard for many customer service applications. Amazon Lex can reduce the time it takes to develop these chatbots and increase their quality. Amazon Lex can also be used to develop interactive assistance. By combining Amazon Lex with other ML services, customers are creating more sophisticated assistance for many different industries. The final example use case is querying databases with a human-like language. Amazon Lex has been combined with other AWS database services to create sophisticated data analysis applications with a human-like language interface. Here are some of the main points you should take away from this module. First, Amazon Transcribe can automatically convert spoken language to text. Amazon Poly can convert written text to spoken language. Amazon Translate can create real-time translation between languages. Amazon Comprehend automates many of the NLP use cases reviewed in this module. And finally, Amazon Lex can create a human-like interface to your applications. Thanks for watching. We'll see you in the next video.  "
Mod03_Sect03_part1.mp4,"Hi and welcome back. This is Section 3 and we're going to cover how to evaluate your data. In this section, we'll look at different data formats and types. We'll also look at how you can visualize and analyze the data before feature engineering. Before you can start running statistics on your data to better understand what you're working with, you need to ensure it's in the right format for analysis. For Amazon SageMaker, algorithms support training with data in CSV format. Many of the tools you'll use to explore, visualize, and analyze the data can also read it in CSV format. Generally speaking, you'll need to have at least some domain knowledge for the problem you're trying to solve with machine learning. For example, if you're developing a model to predict if a set of symptoms indicates a disease, you'd need to know the relationship between the symptoms and the disease. Data typically needs to be in numeric form, so machine learning algorithms can use the data to make predictions. We'll look at ways you can convert text data in the next section. For now, we'll just explore the data and try to gain some insights into the overall data set. One popular open source Python library is Pandas. It can take data in various formats, reformat it, and load it into a tabular representation of your data, presenting it in rows and columns. Some of the formats that Pandas can reformat and load include CSV, Excel, Pickle, and JavaScript object notation or JSON. Pandas also has data analysis and manipulation features, and we'll use them throughout this module. Loading data can be as simple as the example, which pulls in the CSV file from the specified URL. When you load data into Pandas, it's stored as a Pandas data frame. In the Pandas documentation, a data frame is described as a general 2D labeled size mutable tabular structure with potentially heterogeneously typed column. A more helpful way to think of a data frame is to think of it as a spreadsheet or a SQL table. Like a table or spreadsheet, a data frame will have rows, which are also known as instances, and it will have columns, which are also known as attributes. The shape property of a data frame describes the number of rows and columns it has. Each column in a data frame is a series. A series is a one-dimensional labeled array. A series can store data of any type. To learn more about data structures in Pandas, see the Pandas documentation. Along with data, you can load a data frame with row labels and column labels. The row labels are known as an index, and the column labels are known as columns. If you loaded your data from a CSV file with a header row, the columns will be created from the first line of the file. You can change that behavior, however. If you don't have column names in the source file, you can pass them as a parameter. When performing data analysis, it's important to make sure you're using the correct data types. In many cases, Pandas will correctly infer the correct data types when it loads data, and you can move on. If you have domain knowledge or access to a domain expert, they can often identify data type issues. You can use either D types or the info function to obtain information on the column types, as shown in the example. If you don't have the correct data types, you need to figure out why this is the case. Often a numeric column could have been missing data, or it could be a single text value. For example, in the car data set, the number of doors can be 2, 3, 4, or 5 more. After you've analyzed the data, you can convert a column to the correct data type using Pandas. That's it for part 1 of this section. We'll see you again for part 2, where we'll review how to describe your data.  "
Mod05_Sect03_part2.mp4,"Hi, welcome back. We'll continue exploring video analysis by reviewing how to create the training data set. Data sets contain information that's needed to train and test an Amazon recognition custom labels model, such as images, labels, and bounding boxes. You can use images from Amazon S3, or you can upload them from your computer to S3 as part of the process. To train a model, your data set should have at least two labels, with at least ten images per label. Each image in your data set must be labeled. As we mentioned earlier, you can use the Amazon Recognition Custom Labels Console, or Amazon SageMaker Ground Truth to label your images. Again, to train an Amazon Recognition Custom Labels model, your images must be labeled. A label indicates that an image contains an object, scene, or concept. As we mentioned earlier, a data set needs at least two defined labels. Also, each image must have at least one assigned label that identifies the object, scene, or concept in the image. When you apply labels to an image as a whole, these labels are known as image-level labels. They're useful for identifying scenes or concepts that you want to detect. For example, one of the images shows a beach scene from Co-Alina. It's on the island of Oahu in the US State of Hawaii. To train a model to detect beaches, you'd add a beach label that applies to the entire image. You can also apply labels to specific areas of an image that contain an object you want to detect. For example, if you want your model to detect Amazon Echo devices, it must identify the different types of echo devices in an image. The model needs information about where the devices are located in the image. And it needs a corresponding label that identifies the type of the device. This information is known as localization information. The location of the device is expressed as a bounding box. The example objects with bounding boxes' image shows a bounding box that surrounds an Amazon Echo Dot. The image also contains an Amazon Echo without a bounding box. The output of the labeling process will be a manifest file. The manifest file for an image-level label typically contains the label, or class name, along with some metadata about how the image was labeled. For object detection, the manifest contains information about each labeled image. The bounding box identifies where the object is in the image, along with the label that the bounding box belongs to. We've mentioned Amazon SageMaker Ground Truth a few times. We'll now look at what it is and how it might help you. With SageMaker Ground Truth, you can build high quality training datasets for your machine learning models. To use it, create a dataset that needs labeling. You then provide detailed instructions on what needs to be labeled and submit the job. You can decide who processes the images to create a label dataset. You can use workers from the Amazon Mechanical Turk Service, a vendor company, or an internal workforce with machine learning. You can use the label dataset output from SageMaker Ground Truth to train your own models, or you can also use it with Amazon Recognition Custom Labels. SageMaker Ground Truth can use active learning to automate the labeling of your input data. Active learning is a machine learning technique that identifies data that should be labeled by your workers. In SageMaker Ground Truth, this functionality is called automated data labeling. Automated data labeling can reduce the time and cost it takes to label your dataset, compared to using only human workers. When you use automated labeling, you incur Amazon SageMaker training and inference costs. Yes, we just said that you can use machine learning to label the images that you'll then use for machine learning. We'll talk through how this works. When SageMaker Ground Truth starts an automated data labeling job, it selects a random sample of input data or objects and sends it to human workers. When the labeled data is returned, SageMaker Ground Truth uses this data, which is the validation data, to validate the models that were trained for automated data labeling. SageMaker Ground Truth runs a batch transform job using the validated model for inference on the validation data. Batch inference produces a confidence score and quality metric for each object in the validation data. Automated labeling determines if the confidence score for each object, which was produced in step five, meets the required threshold, which was determined in step four. If the confidence score meets the threshold, the expected quality of automatic labeling exceeds the requested level of accuracy. The object is then considered to be automatically labeled. Step six produces a data set of unlabeled data with confidence scores. SageMaker Ground Truth selects data points with low confidence scores from this data set and sends them to human workers for additional labeling. SageMaker Ground Truth then uses the existing human labeled data and the additional human labeled data to train a new model. The process is repeated until the data set is fully labeled or until another stopping condition is met. For example, automatic labeling can stop when you meet your budget for human annotation. We recommend using automated data labeling on large data sets. The minimum number of objects allowed for automated data labeling is 1,250. However, we strongly suggest providing a minimum of 5,000 objects. That's it for part two of this section. We'll see you again for part three where we'll review how to evaluate and improve your model.  "
Mod03_Sect07_part1.mp4,"Hi, welcome back to Module 3. In this section, we'll look at how you can evaluate your model's success in predicting results. At this point, you've trained your models. It's now time to evaluate that model to determine if it will do a good job predicting the target on new and future data. Because future instances have unknown target values, you need to assess how the model will perform on data where you already know the target answer. You'll then use this assessment as a proxy for performance on future data. This is the reason why you hold out a sample of your data for evaluating or testing. An important part of this phase involves choosing the most appropriate metric for your business situation. Think back to the earlier section on problem formulation. During that phase, you define your business problem and outcome. And then you craft a business metric to evaluate success. The model metric you choose at this phase should be linked to that business metric as much as possible. There's often a high correlation between the two metrics. In addition to considering your business problem and success metric, the type of ML problem you're working with will influence the model metric you choose. Throughout the rest of this module, we'll look at examples of common metrics used in classification problems. We'll also look at common metrics used in regression problems. We're going to start by considering a simple binary classification problem. Here's a specific example. Imagine that you have a simple image recognition model that's labeling data as either cat or not cat. After the model's been trained, you can use the test dataset you held back to perform predictions. To help examine the performance of the model, you can compare the predicted values with the actual values. If you plot the values into a table like the example, you can start getting some insights into how well the model performed. In a confusion matrix, you can get a high level comparison of how the predicted class has matched up against the actual classes. If the actual label or class is cat, which is identified as p for positive, and the predicted label or class is also cat, then you have a true positive. This is a good outcome for your model. Similarly, if you have an actual label of not cat, which is identified as n for negative, and the predicted label or class is also not cat, then you have a true negative. This is also a good outcome for your model. In both these cases, your model predicted the correct outcome when it used the testing data. There are two other possible outcomes, and both aren't considered good outcomes. The first one is when the actual class is negative, so you got not cat, but the predicted class is positive or cat. This is called a false positive, because the prediction is positive but incorrect. Finally, there are false negatives. This happened when the actual class is positive, so you got cat, but the predicted class is negative or not cat. That's it for part one of this section. We'll see you again for part two, where we'll review calculating classification metrics.  "
Mod02_Sect05.mp4,"Hi, welcome back. This is Section 5, and we're going to discuss challenges with machine learning. You'll come across many challenges in machine learning. There are a lot of poor quality and inconsistent data available. A significant portion of your job will be getting access to or generating enough good data that's representative of the problem you want to solve. A key issue to watch out for is under or overfitting the model. It's not all about the data, although it mostly is. Do you have data science experience? Is staffing a team of data scientists cost effective? Does management support using machine learning? What does the business landscape look like? Are the problems too complex to formulate into a machine learning problem? Can the resulting model be explained to the business? If it can't be explained, it might not get adopted. What's the cost of building, updating, and operating a machine learning solution? Finally, how does the technology map? Does the business unit have access to the data that's needed? Can the data be secured to meet any regulatory requirements? What tools and frameworks will be used? How will the solution integrate with other systems? These are important questions. To be successful, you'll need to be able to answer and address them. Many machine learning problems can be solved today by using existing models and without substantial machine learning knowledge. We've already talked about the AWS managed services for machine learning. You can add sophisticated machine learning capabilities to your applications with only some basic developer skills for calling APIs. There are other pre-built models you can use or adapt. One example is YOLO, which means you only look once. YOLO is a popular computer vision model. In addition to these scenarios, you can use the AWS marketplace if you'd prefer to buy models and services from independent software vendors instead of developing your own. Here are the key takeaways for this section. First, you'll face many machine learning challenges. The biggest ones that you can directly influence are related to data. You should consider managed services to solve machine learning problems within the domains they support, such as using Amazon recognition for computer vision problems. That's it for this section. We'll see you in the next video.  "
Mod06_Intro.mp4,"Hi, and welcome to Module 6 of AWS Academy Machine Learning, Introduction to Natural Language Processing. In this module, we'll introduce Natural Language Processing, which is also known as NLP. This section includes a description of the major challenges faced by NLP and the overall development process for NLP applications. We'll then review five AWS services you can use to speed up the development of NLP-based applications. After completing this module, you should be able to describe the NLP use cases that are solved by using Managed Amazon ML Services and describe the Managed Amazon ML Services available for NLP. Let's get started.  "
